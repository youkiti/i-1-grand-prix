import { Pool } from 'pg';
import * as dotenv from 'dotenv';
import * as fs from 'fs';
import * as path from 'path';
import { createGateway, generateText } from 'ai';

// Load environment from .env.vercel (with override to ensure it takes precedence over .env)
dotenv.config({ path: path.join(__dirname, '..', '.env.vercel'), override: true });

// ============================================================================
// Configuration
// ============================================================================

const MODEL_NAME = 'google/gemini-3.0-pro-preview';
const MAX_OUTPUT_TOKENS = 64000; // Gemini 3.0 Pro max output limit
const TOKENS_PER_BATCH = 700000; // Target tokens per batch (ç•™ä¿åˆ†ã‚’è€ƒæ…®)
const ESTIMATED_PROMPT_OVERHEAD = 100000; // Prompt template overhead
const PROMPT_VERSION = 'format4-by-hypothesis'; // Update this when changing the prompt

// ============================================================================
// Prompt Templates
// ============================================================================

// Initial report generation prompt
const INITIAL_PROMPT_TEMPLATE = `ã‚ãªãŸã¯æ³•æ¡ˆã«é–¢ã™ã‚‹å¸‚æ°‘ã¨ã®å¯¾è©±ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’åˆ†æã—ã€æ¨ªæ–­çš„ãªè€ƒå¯Ÿã‚’è¡Œã†å°‚é–€å®¶ã§ã™ã€‚
ä¸‹è¨˜ã®æƒ…å ±ã‚’å…ƒã«ã€å¯¾è©±ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã¾ã¨ã‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

## æ³•æ¡ˆã®èƒŒæ™¯

ä»¥ä¸‹ã¯ã€Œ{{interviewTitle}}ã€ã¨ã„ã†ãƒ†ãƒ¼ãƒã§å®Ÿæ–½ã•ã‚ŒãŸå¯¾è©±ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®åˆ†æä¾é ¼ã§ã™ã€‚

ã€æ³•æ¡ˆã®æ¦‚è¦ã€‘
{{interviewDescription}}

{{interviewOverview}}

ã€ä¸»ãªãƒã‚¤ãƒ³ãƒˆã€‘
{{interviewThemes}}

ã€å¯¾è©±ã§æ‰±ã£ãŸè³ªå•ã€‘
{{interviewQuestions}}

ã€å‚è€ƒæƒ…å ±ãƒ»è©³ç´°ã€‘
{{knowledgeContext}}

ã“ã®å¯¾è©±ã¯ä¸Šè¨˜ã®èƒŒæ™¯ã€è³ªå•ã€å‚è€ƒæƒ…å ±ã«åŸºã¥ã„ã¦å®Ÿæ–½ã•ã‚Œã¾ã—ãŸã€‚ã“ã®æ–‡è„ˆã‚’è¸ã¾ãˆã¦ã€ä»¥ä¸‹ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

---

## ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯

ä¸‹è¨˜ã®å¯¾è©±ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆ{{sessionCount}}ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰ã‚’æ¨ªæ–­çš„ã«åˆ†æã—ã€æ³•æ¡ˆã«å¯¾ã™ã‚‹å¸‚æ°‘ã®æ„è¦‹ã€æ‡¸å¿µã€ææ¡ˆã€è³ªå•ãªã©ã‚’æ•´ç†ã—ã¦Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

æ³¨æ„: ã“ã‚Œã¯åˆå›ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã§ã™ã€‚å¾Œã»ã©è¿½åŠ ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã™ã‚‹äºˆå®šãªã®ã§ã€ç¾æ™‚ç‚¹ã§ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ãŸåˆ†æã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

## å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

# {{interviewTitle}} - å¸‚æ°‘å¯¾è©±ãƒ¬ãƒãƒ¼ãƒˆ

## ã¾ã¨ã‚

[å…¨ä½“ã‚’é€šã˜ã¦è¦‹ãˆã¦ããŸä¸»è¦ãªãƒ†ãƒ¼ãƒã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã€é‡è¦ãªç™ºè¦‹ã‚’2-3æ®µè½ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚]

## ä¸»è¦ãªè«–ç‚¹

### è«–ç‚¹1: [æ³•æ¡ˆã«å¯¾ã™ã‚‹ä¸»è¦ãªè«–ç‚¹å]

#### è³›æˆæ„è¦‹ãƒ»æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ
- [æ„è¦‹1]
- [æ„è¦‹2]

#### æ‡¸å¿µãƒ»åå¯¾æ„è¦‹
- [æ‡¸å¿µ1]
- [æ‡¸å¿µ2]

#### ææ¡ˆãƒ»ã‚¢ã‚¤ãƒ‡ã‚¢
- [ææ¡ˆ1]
- [ææ¡ˆ2]

### è«–ç‚¹2: [æ³•æ¡ˆã«å¯¾ã™ã‚‹ä¸»è¦ãªè«–ç‚¹å]

[åŒæ§˜ã«ã€å¯¾è©±ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸä¸»è¦ãªè«–ç‚¹ã”ã¨ã«ç« ç«‹ã¦ã—ã¦åˆ†æçµæœã‚’è¨˜è¿°ã—ã¦ãã ã•ã„]

## ã‚ˆãã‚ã‚‹è³ªå•ã¨å¸‚æ°‘ã®ç–‘å•

[å¯¾è©±ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§é »ç¹ã«å‡ºãŸè³ªå•ã‚„ç–‘å•ç‚¹ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„]

## å½±éŸ¿ã‚’å—ã‘ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã®è¦–ç‚¹

[ç•°ãªã‚‹ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆäº‹æ¥­è€…ã€æ¶ˆè²»è€…ã€è¡Œæ”¿ãªã©ï¼‰ã®è¦–ç‚¹ã‹ã‚‰è¦‹ãŸæ„è¦‹ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„]

---

é‡è¦ãªæ³¨æ„äº‹é …:
1. Markdownãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„
2. HTMLã‚¿ã‚°ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„
3. å¤ªå­—ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ï¼ˆã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯2ã¤ï¼‰ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚æ—¥æœ¬èªã§ã¯æ‹¬å¼§ã®å‡¦ç†ãŒã†ã¾ãã„ã‹ãªã„ã“ã¨ãŒå¤šã„ãŸã‚ã§ã™
4. å‰ç½®ãã‚„èª¬æ˜ã¯ä¸è¦ã§ã€ã€Œ# {{interviewTitle}} - å¸‚æ°‘å¯¾è©±ãƒ¬ãƒãƒ¼ãƒˆã€ã‹ã‚‰å§‹ã‚ã¦ãã ã•ã„
5. å…·ä½“çš„ãªç™ºè¨€ã‚’å¼•ç”¨ã™ã‚‹å ´åˆã¯ã€å¿…ãšã€Œ#ã€ã‚’ä»˜ã‘ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ç•ªå·ã§å‡ºå…¸ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„
6. å€‹ã€…ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¦ç´„ã§ã¯ãªãã€æ¨ªæ–­çš„ãªåˆ†æã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã ã•ã„
7. ã€é‡è¦ã€‘å¯¾è©±å‚åŠ è€…ã®ç”Ÿã®å£°ã‚’ç©æ¥µçš„ã«å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚ã€Œ"ç™ºè¨€å†…å®¹"(#ã‚»ãƒƒã‚·ãƒ§ãƒ³ç•ªå·)ã€ã®å½¢å¼ã§å¤šæ•°å¼•ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒªã‚¢ãƒªãƒ†ã‚£ã¨èª¬å¾—åŠ›ã®ã‚ã‚‹ãƒ¬ãƒãƒ¼ãƒˆã«ã—ã¦ãã ã•ã„
8. ã€é‡è¦ã€‘å¯¾è©±å‚åŠ è€…ã®æ•°é‡çš„æƒ…å ±ã‚„é‡çš„è¡¨ç¾ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚ä»¥ä¸‹ã®ã‚ˆã†ãªè¡¨ç¾ã¯ä½¿ç”¨ç¦æ­¢ã§ã™ï¼š
   - ç¦æ­¢ä¾‹: ã€Œå¤šãã®å‚åŠ è€…ãŒã€ã€Œä¸€éƒ¨ã®äººãŒã€ã€Œã»ã¨ã‚“ã©ã®äººãŒã€ã€Œå¤§åŠãŒã€ã€Œå°‘æ•°ã ãŒã€
   - æ¨å¥¨: ã€Œã‚ã‚‹å‚åŠ è€…ã¯ã€ã€Œã€œã¨ã„ã†æ„è¦‹ãŒã‚ã‚‹ã€ã€Œã€œã¨ã„ã†è¦–ç‚¹ã‚‚ç¤ºã•ã‚ŒãŸã€ãªã©
9. ã€é‡è¦ã€‘ä¾¡å€¤åˆ¤æ–­ã‚„è©•ä¾¡ã‚’å«ã‚€è¡¨ç¾ã‚’é¿ã‘ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®ã‚ˆã†ãªè¡¨ç¾ã¯ä½¿ç”¨ç¦æ­¢ã§ã™ï¼š
   - ç¦æ­¢ä¾‹: ã€Œã•ã‚‰ã«è¸ã¿è¾¼ã¿ã€ã€Œã‚ˆã‚Šé«˜åº¦ãªã€ã€Œæ·±ã„æ´å¯Ÿã€ã€Œå„ªã‚ŒãŸææ¡ˆã€ã€Œé‡è¦åº¦ãŒé«˜ã„ã€
   - æ¨å¥¨: ã€Œã€œã¨ã„ã†æ„è¦‹ãŒã‚ã‚‹ã€ã€Œã€œã¨ã„ã†è¦–ç‚¹ã‚‚ç¤ºã•ã‚ŒãŸã€ãªã©ã€ä¸­ç«‹çš„ãªè¨˜è¿°ã‚’ä½¿ç”¨
10. ã€é‡è¦ã€‘ãƒ¬ãƒˆãƒªãƒƒã‚¯ã‚„ä¸»è¦³çš„ãªä¿®é£¾èªã‚’æ’é™¤ã—ã€å­¦è¡“è«–æ–‡ã®ã‚ˆã†ã«å®¢è¦³çš„ã‹ã¤ç°¡æ½”ã«è¨˜è¿°ã—ã¦ãã ã•ã„
11. ã€é‡è¦ã€‘è¡¨é¢çš„ãªè­°è«–ã«ã¨ã©ã¾ã‚‰ãªã„ã‚ˆã†ã«ã€æ„è¦‹ã«å¯¾ã™ã‚‹åè«–ã‚„å†åè«–ãªã©ã‚‚ç©æ¥µçš„ã«æ§‹é€ åŒ–ã—ãªãŒã‚‰å–ã‚Šä¸Šã’ã¦ãã ã•ã„
12. {{outputLengthGuidance}}

---

## ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ ({{sessionCount}}ã‚»ãƒƒã‚·ãƒ§ãƒ³)

{{sessionsData}}
`;

// Incremental update prompt
const UPDATE_PROMPT_TEMPLATE = `ã‚ãªãŸã¯è¤‡æ•°ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’åˆ†æã—ã€æ¨ªæ–­çš„ãªè€ƒå¯Ÿã‚’è¡Œã†å°‚é–€å®¶ã§ã™ã€‚

## ã‚¿ã‚¹ã‚¯æ¦‚è¦

ä»¥å‰ã«ä½œæˆã—ãŸé›†ç´„ãƒ¬ãƒãƒ¼ãƒˆã«ã€**è¿½åŠ ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼ˆ{{newSessionCount}}ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰ã‚’çµ±åˆ**ã—ã¦ã€ãƒ¬ãƒãƒ¼ãƒˆã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚

**é‡è¦**: æœ€çµ‚çš„ãªãƒ¬ãƒãƒ¼ãƒˆã«ã¯ã€ã©ã®ãƒ‡ãƒ¼ã‚¿ãŒã€Œæ–°ã—ãè¿½åŠ ã•ã‚ŒãŸã€ã‹ã€ã€Œå‰å›ã®ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰ã€ã¨ã„ã£ãŸè¨˜è¿°ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚ã™ã¹ã¦ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä¸€åº¦ã«åˆ†æã—ãŸã‹ã®ã‚ˆã†ã«ã€ä¸€ã¤ã®çµ±åˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

## ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã®èƒŒæ™¯

ä»¥ä¸‹ã¯ã€Œ{{interviewTitle}}ã€ã¨ã„ã†ãƒ†ãƒ¼ãƒã§å®Ÿæ–½ã•ã‚ŒãŸã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã®åˆ†æã§ã™ã€‚

ã€èª¬æ˜ã€‘
{{interviewDescription}}

ã€æ¦‚è¦ã€‘
{{interviewOverview}}

ã€ä¸»ãªãƒ†ãƒ¼ãƒã€‘
{{interviewThemes}}

ã€ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã§æ‰±ã£ãŸè³ªå•ã€‘
{{interviewQuestions}}

ã€å‚è€ƒçŸ¥è­˜ãƒ»ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘
{{knowledgeContext}}

---

## å‰å›ã®ãƒ¬ãƒãƒ¼ãƒˆ

ä»¥ä¸‹ã¯ã€ã“ã‚Œã¾ã§ã«åˆ†æã—ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰ä½œæˆã—ãŸãƒ¬ãƒãƒ¼ãƒˆã§ã™ï¼š

{{previousReport}}

---

## æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ ({{newSessionCount}}ã‚»ãƒƒã‚·ãƒ§ãƒ³)

ä»¥ä¸‹ã®æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’åˆ†æã—ã€ä¸Šè¨˜ã®ãƒ¬ãƒãƒ¼ãƒˆã«çµ±åˆã—ã¦ãã ã•ã„ï¼š

{{sessionsData}}

---

## ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯

1. è¿½åŠ ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€å‰å›ã®ãƒ¬ãƒãƒ¼ãƒˆã«å«ã¾ã‚Œã¦ã„ãªã„ç™ºè¦‹ã€è¦–ç‚¹ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®šã—ã¦ãã ã•ã„
2. å‰å›ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’æ›´æ–°ã—ã¦ãã ã•ã„ï¼š
   - æ–°ã—ã„ç™ºè¦‹ãŒã‚ã‚Œã°ã€è©²å½“ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã«è¿½åŠ 
   - æ–°ã—ã„ã‚«ãƒ†ã‚´ãƒªãŒå¿…è¦ã§ã‚ã‚Œã°è¿½åŠ 
   - æ—¢å­˜ã®è¨˜è¿°ãŒè¿½åŠ ãƒ‡ãƒ¼ã‚¿ã§è£œå¼·ã•ã‚Œã‚‹å ´åˆã¯ã€å¼•ç”¨ã‚’è¿½åŠ 
   - ã€Œã¾ã¨ã‚ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚‚è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚’åæ˜ ã—ã¦æ›´æ–°
3. å‰å›ã®ãƒ¬ãƒãƒ¼ãƒˆã®è‰¯ã„éƒ¨åˆ†ã¯ç¶­æŒã—ã¦ãã ã•ã„ï¼ˆä¸è¦ãªå‰Šé™¤ã‚„æ›¸ãæ›ãˆã¯é¿ã‘ã‚‹ï¼‰

**çµ¶å¯¾ã«å®ˆã‚‹ã¹ããƒ«ãƒ¼ãƒ«:**
- ã€Œæ–°ãŸãªã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚Šã€ã€Œè¿½åŠ ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ã€Œå‰å›ã®ãƒ¬ãƒãƒ¼ãƒˆã§ã¯ã€ã¨ã„ã£ãŸã€åˆ†æãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¤ºå”†ã™ã‚‹è¡¨ç¾ã¯ä¸€åˆ‡ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„
- ã€Œã‚ˆã‚Šå…·ä½“çš„ãªã€ã€Œã•ã‚‰ã«è©³ç´°ãªã€ãªã©ã€ãƒ¬ãƒãƒ¼ãƒˆã®æ›´æ–°å±¥æ­´ã‚’ç¤ºã™è¡¨ç¾ã‚‚é¿ã‘ã¦ãã ã•ã„
- ã™ã¹ã¦ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æœ€åˆã‹ã‚‰ä¸€åº¦ã«åˆ†æã—ãŸã‹ã®ã‚ˆã†ã«è¨˜è¿°ã—ã¦ãã ã•ã„
- å¼•ç”¨ã‚’è¿½åŠ ã™ã‚‹å ´åˆã‚‚ã€å¿…ãšã€Œ#ã€ã‚’ä»˜ã‘ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ç•ªå·ã§å‡ºå…¸ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„
- å‰å›ã®ãƒ¬ãƒãƒ¼ãƒˆã®æ§‹é€ ã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ç¶­æŒã—ã¦ãã ã•ã„
- å‰ç½®ãã‚„èª¬æ˜ã¯ä¸è¦ã§ã€æ›´æ–°ã•ã‚ŒãŸã€Œ# {{interviewTitle}} - é›†ç´„ãƒ¬ãƒãƒ¼ãƒˆã€ã‚’ç›´æ¥å‡ºåŠ›ã—ã¦ãã ã•ã„

æ³¨æ„äº‹é …:
1. Markdownãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„
2. HTMLã‚¿ã‚°ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„
3. å¤ªå­—ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ï¼ˆã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯2ã¤ï¼‰ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚æ—¥æœ¬èªã§ã¯æ‹¬å¼§ã®å‡¦ç†ãŒã†ã¾ãã„ã‹ãªã„ã“ã¨ãŒå¤šã„ãŸã‚ã§ã™
4. å…·ä½“çš„ãªç™ºè¨€ã‚’å¼•ç”¨ã™ã‚‹å ´åˆã¯ã€å¿…ãšã€Œ#ã€ã‚’ä»˜ã‘ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ç•ªå·ã§å‡ºå…¸ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„
5. ã€é‡è¦ã€‘ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼å‚åŠ è€…ã®ç”Ÿã®å£°ã‚’ç©æ¥µçš„ã«å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚ã€Œ"ç™ºè¨€å†…å®¹"(#ã‚»ãƒƒã‚·ãƒ§ãƒ³ç•ªå·)ã€ã®å½¢å¼ã§å¼•ç”¨ã—ã¦ãã ã•ã„
6. ã€é‡è¦ã€‘æ•°é‡çš„æƒ…å ±ã‚„é‡çš„è¡¨ç¾ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ (ã€Œå¤šãã®å‚åŠ è€…ãŒã€ã€Œä¸€éƒ¨ã®äººãŒã€ãªã©ç¦æ­¢)
7. ã€é‡è¦ã€‘ä¾¡å€¤åˆ¤æ–­ã‚„è©•ä¾¡ã‚’å«ã‚€è¡¨ç¾ã‚’é¿ã‘ã¦ãã ã•ã„ (ã€Œæ·±ã„æ´å¯Ÿã€ã€Œå„ªã‚ŒãŸææ¡ˆã€ãªã©ç¦æ­¢)
8. ã€é‡è¦ã€‘ãƒ¬ãƒˆãƒªãƒƒã‚¯ã‚„ä¸»è¦³çš„ãªä¿®é£¾èªã‚’æ’é™¤ã—ã€å­¦è¡“è«–æ–‡ã®ã‚ˆã†ã«å®¢è¦³çš„ã‹ã¤ç°¡æ½”ã«è¨˜è¿°ã—ã¦ãã ã•ã„
9. {{outputLengthGuidance}}

---

æ›´æ–°ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
`;

// Parallel merge prompt - merges multiple independent batch reports into one
const MERGE_PROMPT_TEMPLATE = `ã‚ãªãŸã¯è¤‡æ•°ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’åˆ†æã—ã€æ¨ªæ–­çš„ãªè€ƒå¯Ÿã‚’è¡Œã†å°‚é–€å®¶ã§ã™ã€‚

## ã‚¿ã‚¹ã‚¯æ¦‚è¦

è¤‡æ•°ã®ãƒãƒƒãƒã«åˆ†ã‘ã¦ä¸¦åˆ—ã«åˆ†æã•ã‚ŒãŸã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ã€**ä¸€ã¤ã®çµ±åˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ**ã«ãƒãƒ¼ã‚¸ã—ã¦ãã ã•ã„ã€‚

## ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã®èƒŒæ™¯

ä»¥ä¸‹ã¯ã€Œ{{interviewTitle}}ã€ã¨ã„ã†ãƒ†ãƒ¼ãƒã§å®Ÿæ–½ã•ã‚ŒãŸã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã®åˆ†æã§ã™ã€‚

ã€èª¬æ˜ã€‘
{{interviewDescription}}

ã€æ¦‚è¦ã€‘
{{interviewOverview}}

ã€ä¸»ãªãƒ†ãƒ¼ãƒã€‘
{{interviewThemes}}

ã€ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã§æ‰±ã£ãŸè³ªå•ã€‘
{{interviewQuestions}}

ã€å‚è€ƒçŸ¥è­˜ãƒ»ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘
{{knowledgeContext}}

---

## ãƒãƒƒãƒãƒ¬ãƒãƒ¼ãƒˆ ({{batchCount}}å€‹ã®ãƒ¬ãƒãƒ¼ãƒˆ)

ä»¥ä¸‹ã¯ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’{{batchCount}}å€‹ã®ãƒãƒƒãƒã«åˆ†ã‘ã¦ä¸¦åˆ—ã«åˆ†æã—ãŸçµæœã§ã™ï¼š

{{batchReports}}

---

## ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯

ä¸Šè¨˜ã®{{batchCount}}å€‹ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’çµ±åˆã—ã€ä¸€ã¤ã®åŒ…æ‹¬çš„ãªé›†ç´„ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**é‡è¦ãªæ³¨æ„äº‹é …:**
1. **æƒ…å ±ã®å®Œå…¨ä¿æŒ**: ãƒãƒ¼ã‚¸å‰ã®ãƒ¬ãƒãƒ¼ãƒˆã«ã‚ã£ãŸæƒ…å ±ã‚’å¤±ã‚ãªã„ã§ãã ã•ã„ã€‚ã™ã¹ã¦ã®é‡è¦ãªç™ºè¦‹ã€æŒ‡æ‘˜ã€å¼•ç”¨ã‚’çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã«å«ã‚ã¦ãã ã•ã„
2. **ç”Ÿã®å¼•ç”¨ã‚’ãã®ã¾ã¾å¼•ãç¶™ã**: å„ãƒãƒƒãƒãƒ¬ãƒãƒ¼ãƒˆã«å«ã¾ã‚Œã‚‹å…·ä½“çš„ãªç™ºè¨€ã®å¼•ç”¨ã¯ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ç•ªå·ã¨ã¨ã‚‚ã«ãã®ã¾ã¾çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã«å«ã‚ã¦ãã ã•ã„
3. **é‡è¤‡ã®æ’é™¤**: è¤‡æ•°ã®ãƒãƒƒãƒã§åŒã˜ç™ºè¦‹ã‚„æ„è¦‹ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿ã€ä¸€åº¦ã ã‘è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚ãŸã ã—å¼•ç”¨ã¯ç•°ãªã‚‹ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰ã®ã‚‚ã®ã§ã‚ã‚Œã°è¤‡æ•°æ®‹ã—ã¦ãã ã•ã„
4. **ã‚«ãƒ†ã‚´ãƒªã®å†æ§‹æˆ**: ãƒãƒƒãƒã”ã¨ã®ã‚«ãƒ†ã‚´ãƒªã‚’è¦‹ç›´ã—ã€ã‚ˆã‚Šé©åˆ‡ãªåˆ†é¡ã«å†æ§‹æˆã—ã¦ãã ã•ã„
5. **ç¶²ç¾…æ€§ã®ç¢ºä¿**: ã™ã¹ã¦ã®ãƒãƒƒãƒã‹ã‚‰é‡è¦ãªç™ºè¦‹ã‚’æ¼ã‚‰ã•ãšå«ã‚ã¦ãã ã•ã„ã€‚ãƒãƒ¼ã‚¸å‰ã®ãƒ¬ãƒãƒ¼ãƒˆã§è¨€åŠã•ã‚Œã¦ã„ãŸè«–ç‚¹ã‚„è¦–ç‚¹ãŒã™ã¹ã¦çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã«åæ˜ ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„
6. **ä¸€è²«æ€§ã®ç¶­æŒ**: ãƒˆãƒ¼ãƒ³ã‚„ã‚¹ã‚¿ã‚¤ãƒ«ã‚’çµ±ä¸€ã—ã€ä¸€ã¤ã®ãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦è‡ªç„¶ã«èª­ã‚ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„
7. **æ–‡å­—æ•°ã«ã¤ã„ã¦**: æƒ…å ±ã‚’ä¿æŒã™ã‚‹ãŸã‚ã«æ–‡å­—æ•°ãŒå¢—ãˆã‚‹ã“ã¨ã¯å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚åŒ…æ‹¬çš„ã§è©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„

**çµ¶å¯¾ã«å®ˆã‚‹ã¹ããƒ«ãƒ¼ãƒ«:**
- ã€Œãƒãƒƒãƒ1ã§ã¯ã€ã€Œãƒãƒƒãƒ2ã®åˆ†æã«ã‚ˆã‚‹ã¨ã€ã¨ã„ã£ãŸã€ãƒãƒƒãƒåˆ†å‰²ã‚’ç¤ºå”†ã™ã‚‹è¡¨ç¾ã¯ä¸€åˆ‡ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„
- ã™ã¹ã¦ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æœ€åˆã‹ã‚‰ä¸€åº¦ã«åˆ†æã—ãŸã‹ã®ã‚ˆã†ã«è¨˜è¿°ã—ã¦ãã ã•ã„
- å¼•ç”¨ã™ã‚‹å ´åˆã‚‚ã€å¿…ãšã€Œ#ã€ã‚’ä»˜ã‘ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ç•ªå·ã§å‡ºå…¸ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„
- Markdownãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„
- HTMLã‚¿ã‚°ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„
- å¤ªå­—ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ï¼ˆã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯2ã¤ï¼‰ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„
- å‰ç½®ãã‚„èª¬æ˜ã¯ä¸è¦ã§ã€ã€Œ# {{interviewTitle}} - é›†ç´„ãƒ¬ãƒãƒ¼ãƒˆã€ã‹ã‚‰å§‹ã‚ã¦ãã ã•ã„
- æ•°é‡çš„æƒ…å ±ã‚„é‡çš„è¡¨ç¾ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ (ã€Œå¤šãã®å‚åŠ è€…ãŒã€ãªã©ç¦æ­¢)
- ä¾¡å€¤åˆ¤æ–­ã‚„è©•ä¾¡ã‚’å«ã‚€è¡¨ç¾ã‚’é¿ã‘ã¦ãã ã•ã„ (ã€Œæ·±ã„æ´å¯Ÿã€ãªã©ç¦æ­¢)
- ãƒ¬ãƒˆãƒªãƒƒã‚¯ã‚„ä¸»è¦³çš„ãªä¿®é£¾èªã‚’æ’é™¤ã—ã€å­¦è¡“è«–æ–‡ã®ã‚ˆã†ã«å®¢è¦³çš„ã‹ã¤ç°¡æ½”ã«è¨˜è¿°ã—ã¦ãã ã•ã„
- {{outputLengthGuidance}}

## å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

# {{interviewTitle}} - é›†ç´„ãƒ¬ãƒãƒ¼ãƒˆ

## ã¾ã¨ã‚

[å…¨ä½“ã‚’é€šã˜ã¦è¦‹ãˆã¦ããŸä¸»è¦ãªãƒ†ãƒ¼ãƒã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã€é‡è¦ãªç™ºè¦‹ã‚’2æ®µè½ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚]

## ä»®èª¬1: [äº‹å‰ã«è¨­å®šã—ãŸä»®èª¬]

### æ”¯æŒã™ã‚‹æ„è¦‹
- [æ„è¦‹1]
- [æ„è¦‹2]

### åè«–ãƒ»ç•°ãªã‚‹è¦–ç‚¹
- [åè«–1]
- [åè«–2]

### æ¤œè¨¼çµæœ
[ã“ã®ä»®èª¬ã«å¯¾ã™ã‚‹ç·åˆçš„ãªè©•ä¾¡]

## ä»®èª¬2: [äº‹å‰ã«è¨­å®šã—ãŸä»®èª¬]

[åŒæ§˜ã«ã€äº‹å‰ã«è¨­å®šã—ãŸä»®èª¬ã”ã¨ã«ç« ç«‹ã¦ã—ã¦æ¤œè¨¼çµæœã‚’è¨˜è¿°ã—ã¦ãã ã•ã„]

---

çµ±åˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
`;

// ============================================================================
// Helper Functions
// ============================================================================

function estimateTokens(text: string): number {
  // Rough estimation: 1 token â‰ˆ 4 characters (for Japanese/English mix)
  return Math.ceil(text.length / 4);
}

interface SessionData {
  id: string;
  sessionNumber: number;
  messages: Array<{
    role: 'user' | 'assistant';
    content: string;
    timestamp: string;
  }>;
}

async function fetchSessionData(
  pool: Pool,
  slug: string,
  sessionIds: string[]
): Promise<SessionData[]> {
  if (sessionIds.length === 0) return [];

  console.log(`ğŸ“¥ Fetching ${sessionIds.length} sessions from production...`);

  // Query 1: Get session metadata
  const sessionsResult = await pool.query(`
    SELECT id, session_number
    FROM interview_sessions
    WHERE id = ANY($1) AND slug = $2
    ORDER BY session_number ASC
  `, [sessionIds, slug]);

  const sessionMap = new Map(
    sessionsResult.rows.map(row => [row.id, {
      id: row.id,
      sessionNumber: row.session_number || 0
    }])
  );

  // Query 2: Get all messages
  const messagesResult = await pool.query(`
    SELECT session_id, role, content, timestamp
    FROM messages
    WHERE session_id = ANY($1)
    ORDER BY session_id, timestamp ASC
  `, [sessionIds]);

  // Group messages by session_id
  const messagesBySession = new Map<string, Array<{
    role: 'user' | 'assistant';
    content: string;
    timestamp: string;
  }>>();

  for (const row of messagesResult.rows) {
    if (!messagesBySession.has(row.session_id)) {
      messagesBySession.set(row.session_id, []);
    }
    messagesBySession.get(row.session_id)!.push({
      role: row.role as 'user' | 'assistant',
      content: row.content,
      timestamp: row.timestamp
    });
  }

  // Combine data
  const sessionsData: SessionData[] = [];
  for (const sessionId of sessionIds) {
    const session = sessionMap.get(sessionId);
    const messages = messagesBySession.get(sessionId) || [];
    if (session && messages.length > 0) {
      sessionsData.push({
        id: session.id,
        sessionNumber: session.sessionNumber,
        messages
      });
      console.log(`   âœ“ Session #${session.sessionNumber}: ${messages.length} messages`);
    }
  }

  return sessionsData;
}

function formatSessionsData(sessions: SessionData[]): string {
  return sessions.map(session => {
    const messagesText = session.messages
      .map(msg => `**${msg.role}**: ${msg.content}`)
      .join('\n\n');

    return `### Session #${session.sessionNumber}\n\n${messagesText}`;
  }).join('\n\n---\n\n');
}

// ============================================================================
// Structured Logging Functions
// ============================================================================

interface BatchMetadata {
  batchIndex: number;
  totalBatches: number;
  sessionCount: number;
  sessionIds: string[];
  isFirstBatch: boolean;
  estimatedInputTokens: number;
}

interface ExperimentMetadata {
  experimentId: string;
  timestamp: string;
  slug: string;
  modelName: string;
  promptVersion: string;
  totalBatches: number;
  temperature: number;
  mode: 'sequential' | 'parallel';
  memo?: string;
}

/**
 * Create experiment directory for batch processing
 * Structure:
 *   logs/{experimentId}/
 *     metadata.json           - Overall experiment metadata
 *     batch-1/
 *       metadata.json         - Batch-specific metadata
 *       input-prompt.txt      - Prompt for this batch
 *       input-sessions.json   - Session data for this batch
 *       output-raw.md         - Raw LLM output
 *       output-with-meta.md   - Output with metadata header
 *     batch-2/
 *       ...
 *     final-report.md         - Final merged report
 */
function createExperimentDirectory(experimentId: string): string {
  const logsDir = path.join(process.cwd(), 'logs');
  const experimentDir = path.join(logsDir, experimentId);

  if (!fs.existsSync(logsDir)) {
    fs.mkdirSync(logsDir, { recursive: true });
  }
  if (!fs.existsSync(experimentDir)) {
    fs.mkdirSync(experimentDir, { recursive: true });
  }

  return experimentDir;
}

function createBatchDirectory(experimentDir: string, batchIndex: number): string {
  const batchDir = path.join(experimentDir, `batch-${batchIndex + 1}`);
  if (!fs.existsSync(batchDir)) {
    fs.mkdirSync(batchDir, { recursive: true });
  }
  return batchDir;
}

function saveExperimentMetadata(experimentDir: string, metadata: ExperimentMetadata) {
  const metadataPath = path.join(experimentDir, 'metadata.json');
  fs.writeFileSync(metadataPath, JSON.stringify(metadata, null, 2), 'utf-8');
}

function saveBatchMetadata(batchDir: string, metadata: BatchMetadata, usage?: any) {
  const metadataPath = path.join(batchDir, 'metadata.json');
  const metadataWithUsage = {
    ...metadata,
    usage: usage || null,
  };
  fs.writeFileSync(metadataPath, JSON.stringify(metadataWithUsage, null, 2), 'utf-8');
}

function saveBatchInputs(batchDir: string, prompt: string, sessionsData: SessionData[]) {
  const promptPath = path.join(batchDir, 'input-prompt.txt');
  const sessionsPath = path.join(batchDir, 'input-sessions.json');

  fs.writeFileSync(promptPath, prompt, 'utf-8');
  fs.writeFileSync(sessionsPath, JSON.stringify(sessionsData, null, 2), 'utf-8');
}

function saveBatchOutputs(batchDir: string, rawOutput: string, metadata: BatchMetadata, usage?: any) {
  const rawPath = path.join(batchDir, 'output-raw.md');
  const metaPath = path.join(batchDir, 'output-with-meta.md');

  // Save raw output
  fs.writeFileSync(rawPath, rawOutput, 'utf-8');

  // Save output with metadata header
  const currentDateTime = new Date().toLocaleString('ja-JP');
  const outputWithMeta = `---
Generated: ${currentDateTime}
Batch: ${metadata.batchIndex + 1}/${metadata.totalBatches}
IsFirstBatch: ${metadata.isFirstBatch}
SessionCount: ${metadata.sessionCount}
EstimatedInputTokens: ${metadata.estimatedInputTokens}
ActualInputTokens: ${usage?.promptTokens || 'N/A'}
OutputTokens: ${usage?.completionTokens || 'N/A'}
TotalTokens: ${usage?.totalTokens || 'N/A'}
---

${rawOutput}
`;

  fs.writeFileSync(metaPath, outputWithMeta, 'utf-8');
}

function saveFinalReport(experimentDir: string, report: string, experimentId: string) {
  const finalPath = path.join(experimentDir, 'final-report.md');
  const currentDateTime = new Date().toLocaleString('ja-JP');

  const reportWithMeta = `---
Generated: ${currentDateTime}
ExperimentId: ${experimentId}
Type: Batch-processed aggregate report
---

${report}
`;

  fs.writeFileSync(finalPath, reportWithMeta, 'utf-8');
}

// ============================================================================
// Main Function
// ============================================================================

async function main() {
  const startTime = Date.now();

  // Parse command line arguments
  const args = process.argv.slice(2);
  let slug: string | null = null;
  let memo: string | undefined = undefined;
  let mode: 'sequential' | 'parallel' = 'sequential';

  for (const arg of args) {
    if (arg.startsWith('--slug=')) {
      slug = arg.substring('--slug='.length);
    } else if (arg.startsWith('--memo=')) {
      memo = arg.substring('--memo='.length);
    } else if (arg.startsWith('--mode=')) {
      const modeValue = arg.substring('--mode='.length);
      if (modeValue === 'sequential' || modeValue === 'parallel') {
        mode = modeValue;
      } else {
        console.error(`âŒ Error: Invalid mode "${modeValue}". Must be "sequential" or "parallel"\n`);
        process.exit(1);
      }
    }
  }

  if (!slug) {
    console.error('âŒ Error: --slug parameter is required\n');
    console.log('Usage: npx tsx scripts/20251123_generate-aggregate-report-v2.ts --slug=<slug> [--mode=sequential|parallel] [--memo="memo"]\n');
    console.log('Modes:');
    console.log('  sequential (default) - Process batches one by one, incrementally updating the report');
    console.log('  parallel             - Process all batches in parallel, then merge results (faster)\n');
    console.log('Examples:');
    console.log('  npx tsx scripts/20251123_generate-aggregate-report-v2.ts --slug=bill-of-lading');
    console.log('  npx tsx scripts/20251123_generate-aggregate-report-v2.ts --slug=bill-of-lading --mode=parallel --memo="ä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆ"\n');
    process.exit(1);
  }

  console.log('ğŸš€ Starting aggregate report generation...\n');
  console.log('ğŸ“‹ Configuration:');
  console.log(`   Slug: ${slug}`);
  console.log(`   Mode: ${mode === 'parallel' ? 'Parallel (all batches at once)' : 'Sequential (incremental)'}`);
  console.log(`   Model: ${MODEL_NAME}`);
  console.log(`   Tokens per batch: ${TOKENS_PER_BATCH.toLocaleString()}`);
  console.log(`   Max output tokens: ${MAX_OUTPUT_TOKENS.toLocaleString()}`);
  if (memo) {
    console.log(`   Memo: ${memo}`);
  }
  console.log();

  // Database connection setup
  const connectionString = process.env.POSTGRES_URL_NON_POOLING || process.env.POSTGRES_URL;
  if (!connectionString) {
    console.error('âŒ Error: POSTGRES_URL or POSTGRES_URL_NON_POOLING not found in .env.vercel');
    process.exit(1);
  }

  const apiKey = process.env.AI_GATEWAY_API_KEY;
  if (!apiKey) {
    console.error('âŒ Error: AI_GATEWAY_API_KEY not found in .env.vercel');
    process.exit(1);
  }

  const isLocalSupabase = connectionString?.includes('localhost') || connectionString?.includes('127.0.0.1');
  const sslConfig = isLocalSupabase ? false : (connectionString ? { rejectUnauthorized: false } : false);

  const pool = new Pool({
    connectionString: connectionString?.replace(/[?&]sslmode=[^&]*/g, ''),
    ssl: sslConfig
  });

  // Add error handler to prevent unhandled error events
  pool.on('error', (err) => {
    console.error('âš ï¸  Pool error (ignored):', err.message);
  });

  try {
    console.log('ğŸ”Œ Connecting to production database...');

    // Get interview config
    const configResult = await pool.query(`
      SELECT title, description, landing_page, questions, knowledge_source_text FROM interview_configs WHERE slug = $1 LIMIT 1
    `, [slug]);
    console.log(`   Config query completed: ${configResult.rows.length} rows`);

    if (configResult.rows.length === 0) {
      throw new Error(`Interview config not found for slug: ${slug}`);
    }

    const interviewTitle = configResult.rows[0].title;
    const interviewDescription = configResult.rows[0].description || '';
    const landingPage = configResult.rows[0].landing_page || {};
    const questions = configResult.rows[0].questions || [];
    const knowledgeSourceText = configResult.rows[0].knowledge_source_text || '';

    // Extract data from landing_page
    const interviewOverview = landingPage.overview || 'ï¼ˆæ¦‚è¦ãªã—ï¼‰';
    const interviewThemes = (landingPage.themes || [])
      .map((theme: string, idx: number) => `${idx + 1}. ${theme}`)
      .join('\n') || 'ï¼ˆãƒ†ãƒ¼ãƒãªã—ï¼‰';

    const interviewQuestions = (questions || [])
      .map((q: any, idx: number) => {
        const topic = q.topic || 'ï¼ˆãƒˆãƒ”ãƒƒã‚¯ãªã—ï¼‰';
        const mainQuestion = q.mainQuestion || '';
        return `${idx + 1}. **${topic}**: ${mainQuestion}`;
      })
      .join('\n') || 'ï¼ˆè³ªå•ãªã—ï¼‰';

    const knowledgeContext = knowledgeSourceText || 'ï¼ˆå‚è€ƒçŸ¥è­˜ãªã—ï¼‰';

    console.log(`   Interview: ${interviewTitle}\n`);

    // Get all sessions for this interview (sorted by message count DESC)
    const allSessionsResult = await pool.query(`
      SELECT
        s.id,
        s.session_number,
        COUNT(m.id) as message_count
      FROM interview_sessions s
      LEFT JOIN messages m ON s.id = m.session_id
      WHERE s.slug = $1
      GROUP BY s.id, s.session_number
      HAVING COUNT(m.id) >= 7
      ORDER BY COUNT(m.id) DESC
    `, [slug]);

    const allSessionIds = allSessionsResult.rows.map(row => row.id);
    const allMessageCounts = new Map(allSessionsResult.rows.map(row => [row.id, row.message_count]));

    console.log(`ğŸ“Š Total sessions available: ${allSessionIds.length}\n`);

    if (allSessionIds.length === 0) {
      console.error('âŒ No sessions found with minimum 7 messages');
      process.exit(1);
    }

    // Split sessions into batches
    const batches: string[][] = [];
    let currentBatch: string[] = [];
    let currentTokens = ESTIMATED_PROMPT_OVERHEAD;

    for (const sessionId of allSessionIds) {
      const messageCount = allMessageCounts.get(sessionId) || 0;
      const estimatedSessionTokens = messageCount * 350; // Rough estimate per message

      if (currentTokens + estimatedSessionTokens > TOKENS_PER_BATCH && currentBatch.length > 0) {
        // Start new batch
        batches.push(currentBatch);
        currentBatch = [sessionId];
        currentTokens = ESTIMATED_PROMPT_OVERHEAD + estimatedSessionTokens;
      } else {
        currentBatch.push(sessionId);
        currentTokens += estimatedSessionTokens;
      }
    }

    if (currentBatch.length > 0) {
      batches.push(currentBatch);
    }

    console.log(`ğŸ“¦ Split into ${batches.length} batches:\n`);
    batches.forEach((batch, index) => {
      console.log(`   Batch ${index + 1}: ${batch.length} sessions`);
    });
    console.log();

    // Create experiment directory
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-').substring(0, 19);
    const experimentId = `${slug}-${PROMPT_VERSION}-${timestamp}`;

    const experimentMetadata: ExperimentMetadata = {
      experimentId,
      timestamp: new Date().toISOString(),
      slug,
      modelName: MODEL_NAME,
      promptVersion: PROMPT_VERSION,
      totalBatches: batches.length,
      temperature: 0.3,
      mode,
      ...(memo && { memo }),
    };

    console.log('ğŸ“ Creating experiment directory...');
    const experimentDir = createExperimentDirectory(experimentId);
    saveExperimentMetadata(experimentDir, experimentMetadata);
    console.log(`   ${experimentDir}\n`);

    // API key
    const apiKey = process.env.AI_GATEWAY_API_KEY;
    if (!apiKey) {
      throw new Error('AI_GATEWAY_API_KEY not found in environment');
    }

    const gateway = createGateway({ apiKey });

    // Close initial connection
    await pool.end();

    let currentReport: string = '';

    // Choose processing mode
    if (mode === 'parallel') {
      // Parallel mode: process all batches simultaneously, then merge
      currentReport = await processParallelMode(
        batches,
        experimentDir,
        gateway,
        slug,
        interviewTitle,
        interviewDescription,
        interviewOverview,
        interviewThemes,
        interviewQuestions,
        knowledgeContext,
        connectionString,
        sslConfig
      );
    } else {
      // Sequential mode: process batches one by one (original behavior)
      currentReport = await processSequentialMode(
        batches,
        experimentDir,
        gateway,
        slug,
        interviewTitle,
        interviewDescription,
        interviewOverview,
        interviewThemes,
        interviewQuestions,
        knowledgeContext,
        connectionString,
        sslConfig
      );
    }

    // Save final report to experiment directory
    console.log('\nğŸ“ Saving final report to experiment directory...');
    saveFinalReport(experimentDir, currentReport, experimentId);
    console.log(`   âœ“ final-report.md\n`);

    // Also save to backups directory for backward compatibility
    const finalTimestamp = new Date().toISOString().replace(/[:.]/g, '-').substring(0, 19);
    const backupsDir = path.join(process.cwd(), 'backups');

    if (!fs.existsSync(backupsDir)) {
      fs.mkdirSync(backupsDir, { recursive: true });
    }

    const outputPath = path.join(backupsDir, `aggregate-report-v2-${slug}-${finalTimestamp}.md`);

    // Add metadata
    const currentDateTime = new Date().toLocaleString('ja-JP');
    const totalSessions = allSessionIds.length;
    const reportWithMetadata = `---
Generated: ${currentDateTime}
Model: ${MODEL_NAME}
Slug: ${slug}
SessionCount: ${totalSessions}
Method: ${mode === 'parallel' ? 'Parallel' : 'Incremental'} (${batches.length} batches)
PromptVersion: ${PROMPT_VERSION}
ExperimentId: ${experimentId}
---

${currentReport}
`;

    fs.writeFileSync(outputPath, reportWithMetadata, 'utf-8');

    console.log('âœ… Final report generated!\n');
    console.log('ğŸ“Š Statistics:');
    console.log(`   Total sessions: ${totalSessions}`);
    console.log(`   Batches processed: ${batches.length}`);
    console.log(`   Final output size: ${(reportWithMetadata.length / 1024).toFixed(2)} KB`);
    console.log(`   Processing time: ${((Date.now() - startTime) / 1000).toFixed(2)}s\n`);
    console.log('ğŸ“ Results saved:');
    console.log(`   Experiment: logs/${experimentId}/`);
    console.log(`   Backup: ${outputPath}\n`);
    console.log('âœ¨ Done!\n');

  } catch (error) {
    console.error('âŒ Error:', error);
    throw error;
  }
}

// ============================================================================
// Sequential Mode (original implementation)
// ============================================================================

async function processSequentialMode(
  batches: string[][],
  experimentDir: string,
  gateway: any,
  slug: string,
  interviewTitle: string,
  interviewDescription: string,
  interviewOverview: string,
  interviewThemes: string,
  interviewQuestions: string,
  knowledgeContext: string,
  connectionString: string | undefined,
  sslConfig: any
): Promise<string> {
  let currentReport: string = '';

  // Process each batch
  for (let batchIndex = 0; batchIndex < batches.length; batchIndex++) {
      const batch = batches[batchIndex];
      const isFirstBatch = batchIndex === 0;

      console.log(`\n${'='.repeat(60)}`);
      console.log(`ğŸ“¦ Processing Batch ${batchIndex + 1}/${batches.length}`);
      console.log(`${'='.repeat(60)}\n`);

      // Create new pool for this batch
      const batchPool = new Pool({
        connectionString: connectionString?.replace(/[?&]sslmode=[^&]*/g, ''),
        ssl: sslConfig
      });

      // Add error handler to prevent unhandled error events
      batchPool.on('error', (err) => {
        console.error('âš ï¸  Batch pool error (ignored):', err.message);
      });

      // Fetch session data for this batch
      const sessionsData = await fetchSessionData(batchPool, slug, batch);
      const sessionsText = formatSessionsData(sessionsData);

      // Close connection before LLM call (to avoid timeout during long LLM processing)
      await batchPool.end();

      // Prepare prompt
      let prompt: string;
      const outputLengthGuidance = 'Target output length: 10,000-20,000 characters per batch. Provide detailed analysis with extensive quotations.';

      if (isFirstBatch) {
        // Initial report generation
        console.log('ğŸ¤– Generating initial report...\n');
        prompt = INITIAL_PROMPT_TEMPLATE
          .replace(/\{\{interviewTitle\}\}/g, interviewTitle)
          .replace(/\{\{interviewDescription\}\}/g, interviewDescription)
          .replace(/\{\{interviewOverview\}\}/g, interviewOverview)
          .replace(/\{\{interviewThemes\}\}/g, interviewThemes)
          .replace(/\{\{interviewQuestions\}\}/g, interviewQuestions)
          .replace(/\{\{knowledgeContext\}\}/g, knowledgeContext)
          .replace(/\{\{sessionCount\}\}/g, String(sessionsData.length))
          .replace(/\{\{sessionsData\}\}/g, sessionsText)
          .replace(/\{\{outputLengthGuidance\}\}/g, outputLengthGuidance);
      } else {
        // Incremental update
        console.log(`ğŸ”„ Updating report with new sessions...\n`);
        prompt = UPDATE_PROMPT_TEMPLATE
          .replace(/\{\{interviewTitle\}\}/g, interviewTitle)
          .replace(/\{\{interviewDescription\}\}/g, interviewDescription)
          .replace(/\{\{interviewOverview\}\}/g, interviewOverview)
          .replace(/\{\{interviewThemes\}\}/g, interviewThemes)
          .replace(/\{\{interviewQuestions\}\}/g, interviewQuestions)
          .replace(/\{\{knowledgeContext\}\}/g, knowledgeContext)
          .replace(/\{\{previousReport\}\}/g, currentReport)
          .replace(/\{\{newSessionCount\}\}/g, String(sessionsData.length))
          .replace(/\{\{sessionsData\}\}/g, sessionsText)
          .replace(/\{\{outputLengthGuidance\}\}/g, outputLengthGuidance);
      }

      const estimatedInputTokens = estimateTokens(prompt);
      console.log(`   Input tokens (estimated): ${estimatedInputTokens.toLocaleString()}`);
      console.log(`   Sessions in batch: ${sessionsData.length}`);

      // Create batch directory and save inputs
      console.log('   ğŸ“ Creating batch directory...');
      const batchDir = createBatchDirectory(experimentDir, batchIndex);

      const batchMetadata: BatchMetadata = {
        batchIndex,
        totalBatches: batches.length,
        sessionCount: sessionsData.length,
        sessionIds: batch,
        isFirstBatch,
        estimatedInputTokens,
      };

      saveBatchInputs(batchDir, prompt, sessionsData);
      console.log(`   ğŸ’¾ Saved: input-prompt.txt, input-sessions.json`);

      console.log(`   ğŸ¤– Calling Gemini 3.0 Pro...\n`);

      // Call LLM
      const result = await generateText({
        model: gateway(MODEL_NAME),
        prompt,
        temperature: 0.3,
        maxRetries: 0,
        abortSignal: AbortSignal.timeout(600000), // 10 minutes
      });

      currentReport = result.text;

      // Save batch outputs
      saveBatchOutputs(batchDir, currentReport, batchMetadata, result.usage);
      saveBatchMetadata(batchDir, batchMetadata, result.usage);
      console.log(`   ğŸ’¾ Saved: output-raw.md, output-with-meta.md, metadata.json`);

      console.log(`âœ… Batch ${batchIndex + 1} completed!`);
      if (result.usage) {
        const usage = result.usage as any;
        console.log(`   Output tokens: ${usage.completionTokens?.toLocaleString() || 'N/A'}`);
        console.log(`   Total tokens: ${usage.totalTokens?.toLocaleString() || 'N/A'}`);
      }
      console.log(`   Output size: ${(currentReport.length / 1024).toFixed(2)} KB`);
    }

    return currentReport;
}

// ============================================================================
// Parallel Mode (concurrent batch processing with final merge)
// ============================================================================

async function processParallelMode(
  batches: string[][],
  experimentDir: string,
  gateway: any,
  slug: string,
  interviewTitle: string,
  interviewDescription: string,
  interviewOverview: string,
  interviewThemes: string,
  interviewQuestions: string,
  knowledgeContext: string,
  connectionString: string | undefined,
  sslConfig: any
): Promise<string> {
  console.log('ğŸš€ Processing all batches in parallel...\n');

  // Process all batches concurrently
  const batchPromises = batches.map(async (batch, batchIndex) => {
    console.log(`\n${'='.repeat(60)}`);
    console.log(`ğŸ“¦ Starting Batch ${batchIndex + 1}/${batches.length} (parallel)`);
    console.log(`${'='.repeat(60)}\n`);

    // Create new pool for this batch
    const batchPool = new Pool({
      connectionString: connectionString?.replace(/[?&]sslmode=[^&]*/g, ''),
      ssl: sslConfig
    });

    // Add error handler
    batchPool.on('error', (err) => {
      console.error(`âš ï¸  Batch ${batchIndex + 1} pool error (ignored):`, err.message);
    });

    // Fetch session data for this batch
    const sessionsData = await fetchSessionData(batchPool, slug, batch);
    const sessionsText = formatSessionsData(sessionsData);

    // Close connection before LLM call
    await batchPool.end();

    // Prepare prompt (always use INITIAL template for parallel mode)
    const outputLengthGuidance = 'Target output length: 10,000-20,000 characters per batch. Provide detailed analysis with extensive quotations.';

    console.log(`   ğŸ¤– [Batch ${batchIndex + 1}] Generating report in parallel...\n`);
    const prompt = INITIAL_PROMPT_TEMPLATE
      .replace(/\{\{interviewTitle\}\}/g, interviewTitle)
      .replace(/\{\{interviewDescription\}\}/g, interviewDescription)
      .replace(/\{\{interviewOverview\}\}/g, interviewOverview)
      .replace(/\{\{interviewThemes\}\}/g, interviewThemes)
      .replace(/\{\{interviewQuestions\}\}/g, interviewQuestions)
      .replace(/\{\{knowledgeContext\}\}/g, knowledgeContext)
      .replace(/\{\{sessionCount\}\}/g, String(sessionsData.length))
      .replace(/\{\{sessionsData\}\}/g, sessionsText)
      .replace(/\{\{outputLengthGuidance\}\}/g, outputLengthGuidance);

    const estimatedInputTokens = estimateTokens(prompt);
    console.log(`   [Batch ${batchIndex + 1}] Input tokens (estimated): ${estimatedInputTokens.toLocaleString()}`);
    console.log(`   [Batch ${batchIndex + 1}] Sessions in batch: ${sessionsData.length}`);

    // Create batch directory and save inputs
    console.log(`   [Batch ${batchIndex + 1}] ğŸ“ Creating batch directory...`);
    const batchDir = createBatchDirectory(experimentDir, batchIndex);

    const batchMetadata: BatchMetadata = {
      batchIndex,
      totalBatches: batches.length,
      sessionCount: sessionsData.length,
      sessionIds: batch,
      isFirstBatch: batchIndex === 0,
      estimatedInputTokens,
    };

    saveBatchInputs(batchDir, prompt, sessionsData);
    console.log(`   [Batch ${batchIndex + 1}] ğŸ’¾ Saved: input-prompt.txt, input-sessions.json`);

    console.log(`   [Batch ${batchIndex + 1}] ğŸ¤– Calling Gemini 3.0 Pro...\n`);

    // Call LLM
    const result = await generateText({
      model: gateway(MODEL_NAME),
      prompt,
      temperature: 0.3,
      maxRetries: 0,
      abortSignal: AbortSignal.timeout(600000), // 10 minutes
    });

    const batchReport = result.text;

    // Save batch outputs
    saveBatchOutputs(batchDir, batchReport, batchMetadata, result.usage);
    saveBatchMetadata(batchDir, batchMetadata, result.usage);
    console.log(`   [Batch ${batchIndex + 1}] ğŸ’¾ Saved: output-raw.md, output-with-meta.md, metadata.json`);

    console.log(`âœ… Batch ${batchIndex + 1} completed!`);
    if (result.usage) {
      const usage = result.usage as any;
      console.log(`   [Batch ${batchIndex + 1}] Output tokens: ${usage.completionTokens?.toLocaleString() || 'N/A'}`);
      console.log(`   [Batch ${batchIndex + 1}] Total tokens: ${usage.totalTokens?.toLocaleString() || 'N/A'}`);
    }
    console.log(`   [Batch ${batchIndex + 1}] Output size: ${(batchReport.length / 1024).toFixed(2)} KB`);

    return batchReport;
  });

  // Wait for all batches to complete
  console.log('\nâ³ Waiting for all parallel batches to complete...\n');
  const batchReports = await Promise.all(batchPromises);
  console.log('âœ… All batches completed!\n');

  // Merge all batch reports using MERGE_PROMPT
  console.log(`\n${'='.repeat(60)}`);
  console.log(`ğŸ”€ Merging ${batchReports.length} batch reports...`);
  console.log(`${'='.repeat(60)}\n`);

  const combinedBatchReports = batchReports
    .map((report, idx) => `## ãƒãƒƒãƒ ${idx + 1} ã®åˆ†æçµæœ\n\n${report}`)
    .join('\n\n' + '='.repeat(60) + '\n\n');

  const mergeOutputLengthGuidance = 'Target output length: Comprehensive report with all information from batch reports preserved. Length may exceed individual batch reports to ensure no information is lost.';

  const mergePrompt = MERGE_PROMPT_TEMPLATE
    .replace(/\{\{interviewTitle\}\}/g, interviewTitle)
    .replace(/\{\{interviewDescription\}\}/g, interviewDescription)
    .replace(/\{\{interviewOverview\}\}/g, interviewOverview)
    .replace(/\{\{interviewThemes\}\}/g, interviewThemes)
    .replace(/\{\{interviewQuestions\}\}/g, interviewQuestions)
    .replace(/\{\{knowledgeContext\}\}/g, knowledgeContext)
    .replace(/\{\{batchCount\}\}/g, String(batchReports.length))
    .replace(/\{\{batchReports\}\}/g, combinedBatchReports)
    .replace(/\{\{outputLengthGuidance\}\}/g, mergeOutputLengthGuidance);

  const estimatedMergeTokens = estimateTokens(mergePrompt);
  console.log(`   Merge input tokens (estimated): ${estimatedMergeTokens.toLocaleString()}`);
  console.log(`   ğŸ¤– Calling Gemini 3.0 Pro for merge...\n`);

  // Call LLM for merge
  const mergeResult = await generateText({
    model: gateway(MODEL_NAME),
    prompt: mergePrompt,
    temperature: 0.3,
    maxRetries: 0,
    abortSignal: AbortSignal.timeout(600000), // 10 minutes
  });

  const finalReport = mergeResult.text;

  console.log(`âœ… Merge completed!`);
  if (mergeResult.usage) {
    const usage = mergeResult.usage as any;
    console.log(`   Output tokens: ${usage.completionTokens?.toLocaleString() || 'N/A'}`);
    console.log(`   Total tokens: ${usage.totalTokens?.toLocaleString() || 'N/A'}`);
  }
  console.log(`   Output size: ${(finalReport.length / 1024).toFixed(2)} KB`);

  // Save merge artifacts
  const mergeDir = path.join(experimentDir, 'merge');
  if (!fs.existsSync(mergeDir)) {
    fs.mkdirSync(mergeDir, { recursive: true });
  }

  fs.writeFileSync(path.join(mergeDir, 'input-prompt.txt'), mergePrompt, 'utf-8');
  fs.writeFileSync(path.join(mergeDir, 'output-raw.md'), finalReport, 'utf-8');

  const mergeMetadata = {
    batchCount: batchReports.length,
    estimatedInputTokens: estimatedMergeTokens,
    usage: mergeResult.usage
  };
  fs.writeFileSync(path.join(mergeDir, 'metadata.json'), JSON.stringify(mergeMetadata, null, 2), 'utf-8');

  console.log(`   ğŸ’¾ Saved merge artifacts to: merge/\n`);

  return finalReport;
}

main().catch(error => {
  console.error('Fatal error:', error);
  process.exit(1);
});

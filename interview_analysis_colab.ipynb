{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ğŸ¯ ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ„ãƒ¼ãƒ«\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€è¤‡æ•°ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æ¨ªæ–­çš„ã«åˆ†æã—ã€ä»®èª¬æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚\n",
    "\n",
    "**å…ƒã®TypeScriptã‚³ãƒ¼ãƒ‰:** `20251123_test-format4-ai-plan-hypothesis.ts`\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "header"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "#@title ğŸ“– ä½¿ã„æ–¹ã‚¬ã‚¤ãƒ‰\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "guide = \"\"\"\n",
    "# ğŸ“ ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ä½¿ã„æ–¹\n",
    "\n",
    "## ã‚¹ãƒ†ãƒƒãƒ—1: APIã‚­ãƒ¼ã®è¨­å®š ğŸ”‘\n",
    "\n",
    "1. å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ã® **ğŸ”‘ (Secrets)** ã‚¢ã‚¤ã‚³ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯\n",
    "2. ã€ŒAdd new secretã€ã‚’ã‚¯ãƒªãƒƒã‚¯\n",
    "3. Name: `GOOGLE_API_KEY`\n",
    "4. Value: ã‚ãªãŸã® Google API Key ã‚’è²¼ã‚Šä»˜ã‘\n",
    "5. ã€ŒNotebook accessã€ã‚’ONã«ã™ã‚‹\n",
    "\n",
    "> **APIã‚­ãƒ¼ã®å–å¾—æ–¹æ³•:**  \n",
    "> [Google AI Studio](https://aistudio.google.com/app/apikey) ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã€ŒCreate API Keyã€\n",
    "\n",
    "---\n",
    "\n",
    "## ã‚¹ãƒ†ãƒƒãƒ—2: ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ â–¶ï¸\n",
    "\n",
    "1. **ğŸ“¦ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—** â†’ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "2. **ğŸ“ CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰** â†’ ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "3. **âš™ï¸ è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿** â†’ ãƒ¢ãƒ‡ãƒ«ã‚„slugã‚’è¨­å®š\n",
    "4. **ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ** (3ã¤ã®ã‚»ãƒ«) â†’ å¿…è¦ãªã‚‰ç·¨é›†\n",
    "5. **ğŸ› ï¸ ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°** â†’ é–¢æ•°ã‚’å®šç¾©\n",
    "6. **ğŸ”„ å‡¦ç†ãƒ¢ãƒ¼ãƒ‰é–¢æ•°** â†’ Sequential/Parallelå‡¦ç†ã‚’å®šç¾©\n",
    "7. **â–¶ï¸ ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ** â†’ å®Ÿéš›ã®åˆ†æã‚’å®Ÿè¡Œ\n",
    "\n",
    "---\n",
    "\n",
    "## ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰âœï¸\n",
    "\n",
    "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã‚»ãƒ«ï¼ˆã‚»ãƒ«4-6ï¼‰ã‚’ç›´æ¥ç·¨é›†ã§ãã¾ã™ã€‚\n",
    "\n",
    "- **ã‚»ãƒ«4**: åˆå›ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "- **ã‚»ãƒ«5**: æ›´æ–°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆé€æ¬¡å‡¦ç†ç”¨ï¼‰\n",
    "- **ã‚»ãƒ«6**: ãƒãƒ¼ã‚¸ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰\n",
    "\n",
    "ç·¨é›†å¾Œã€ãã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚° ğŸ”§\n",
    "\n",
    "### APIã‚­ãƒ¼ã‚¨ãƒ©ãƒ¼\n",
    "```\n",
    "âŒ Google API Key ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ\n",
    "```\n",
    "â†’ Secrets ã« `GOOGLE_API_KEY` ãŒç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "\n",
    "### CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„\n",
    "```\n",
    "âš ï¸ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\n",
    "```\n",
    "â†’ `MESSAGES_CSV_PATH` ã®ãƒ‘ã‚¹ã‚’ä¿®æ­£\n",
    "\n",
    "### ãƒ¡ãƒ¢ãƒªä¸è¶³\n",
    "â†’ ã‚ˆã‚Šå°ã•ã„ `TOKENS_PER_BATCH` ã‚’è©¦ã™ã‹ã€`MIN_MESSAGES` ã‚’å¢—ã‚„ã—ã¦ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ã‚’æ¸›ã‚‰ã™\n",
    "\n",
    "---\n",
    "\n",
    "**æº–å‚™ãŒã§ããŸã‚‰ã€æ¬¡ã®ã‚»ãƒ«ã‹ã‚‰é †ç•ªã«å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼** ğŸš€\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(guide))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title ğŸ“¦ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "\n",
    "print(\"ğŸ“¦ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "\n",
    "# Google Generative AI SDK ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install -q google-generativeai pandas\n",
    "\n",
    "print(\"âœ… ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\\n\")\n",
    "\n",
    "# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "from google.colab import files, userdata\n",
    "import time\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "print(\"âœ… ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "install"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title ğŸ“ CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "#@markdown ä»¥ä¸‹ã®3ã¤ã®æ–¹æ³•ã‹ã‚‰é¸ã‚“ã§ãã ã•ã„ï¼š\n",
    "\n",
    "upload_method = \"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\" #@param [\"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\", \"Google Drive\", \"ã™ã§ã«é…ç½®æ¸ˆã¿\"]\n",
    "\n",
    "if upload_method == \"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\":\n",
    "    print(\"ğŸ“¤ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„...\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # data/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "    os.makedirs('/content/data', exist_ok=True)\n",
    "    \n",
    "    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜\n",
    "    for filename in uploaded.keys():\n",
    "        if 'messages' in filename:\n",
    "            target_path = f'/content/data/{filename}'\n",
    "            with open(target_path, 'wb') as f:\n",
    "                f.write(uploaded[filename])\n",
    "            print(f\"âœ… {target_path} ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "\n",
    "elif upload_method == \"Google Drive\":\n",
    "    from google.colab import drive\n",
    "    \n",
    "    print(\"ğŸ“‚ Google Drive ã‚’ãƒã‚¦ãƒ³ãƒˆä¸­...\")\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    print(\"âœ… ãƒã‚¦ãƒ³ãƒˆå®Œäº†\")\n",
    "    print(\"ğŸ’¡ æ¬¡ã®ã‚»ãƒ«ã§ MESSAGES_CSV_PATH ã‚’æ­£ã—ã„ãƒ‘ã‚¹ã«ä¿®æ­£ã—ã¦ãã ã•ã„\")\n",
    "    print(\"   ä¾‹: /content/drive/MyDrive/data/bill-of-lading_messages.csv\")\n",
    "\n",
    "else:\n",
    "    print(\"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ãŒã™ã§ã«é…ç½®ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¾ã™\")\n",
    "    print(\"ğŸ’¡ æ¬¡ã®ã‚»ãƒ«ã§ MESSAGES_CSV_PATH ãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "upload"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title âš™ï¸ è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆåˆå¿ƒè€…å‘ã‘ï¼‰\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### ğŸ”‘ APIè¨­å®š\n",
    "#@markdown Google APIã‚­ãƒ¼ã¯ Colab ã®ã€ŒğŸ”‘ Secretsã€ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã™\n",
    "#@markdown ï¼ˆå·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®éµã‚¢ã‚¤ã‚³ãƒ³ â†’ `GOOGLE_API_KEY` ã¨ã„ã†åå‰ã§APIã‚­ãƒ¼ã‚’ç™»éŒ²ï¼‰\n",
    "\n",
    "try:\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "    print(\"âœ… Google API Key ã‚’ Colab Secrets ã‹ã‚‰å–å¾—ã—ã¾ã—ãŸ\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Google API Key ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "    print(\"ğŸ’¡ å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ğŸ”‘ã‚¢ã‚¤ã‚³ãƒ³ã‹ã‚‰ 'GOOGLE_API_KEY' ã‚’ç™»éŒ²ã—ã¦ãã ã•ã„\")\n",
    "    raise e\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### ğŸ¤– ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
    "MODEL_NAME = \"gemini-flash-lite-latest\" #@param [\"gemini-flash-lite-latest\", \"models/gemini-3-pro-preview\", \"gemini-2.0-flash-exp\", \"gemini-1.5-pro\", \"gemini-1.5-flash\"] {allow-input: true}\n",
    "MAX_OUTPUT_TOKENS = 64000 #@param {type:\"integer\"}\n",
    "TEMPERATURE = 0.3 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### ğŸ“‚ ãƒ‡ãƒ¼ã‚¿è¨­å®š\n",
    "slug = \"bill-of-lading\" #@param {type:\"string\"}\n",
    "MIN_MESSAGES = 7 #@param {type:\"integer\"}\n",
    "#@markdown â†‘ ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«å«ã¾ã‚Œã‚‹æœ€å°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ï¼ˆã“ã‚Œæœªæº€ã¯é™¤å¤–ï¼‰\n",
    "\n",
    "MESSAGES_CSV_PATH = \"/content/data/bill-of-lading_messages.csv\" #@param {type:\"string\"}\n",
    "#@markdown â†‘ messages.csv ã®ãƒ‘ã‚¹\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### ğŸ”„ å‡¦ç†ãƒ¢ãƒ¼ãƒ‰\n",
    "mode = \"sequential\" #@param [\"sequential\", \"parallel\"]\n",
    "#@markdown - **sequential**: é€æ¬¡å‡¦ç†ï¼ˆãƒãƒƒãƒã‚’1ã¤ãšã¤å‡¦ç†ï¼‰\n",
    "#@markdown - **parallel**: ä¸¦åˆ—å‡¦ç†ï¼ˆå…¨ãƒãƒƒãƒåŒæ™‚å‡¦ç†å¾Œã«ãƒãƒ¼ã‚¸ï¼‰\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### âš¡ ãƒãƒƒãƒè¨­å®š\n",
    "TOKENS_PER_BATCH = 700000 #@param {type:\"integer\"}\n",
    "ESTIMATED_PROMPT_OVERHEAD = 100000 #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### ğŸ“ ãƒ¡ãƒ¢ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "memo = \"\" #@param {type:\"string\"}\n",
    "\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª\n",
    "import os\n",
    "if os.path.exists(MESSAGES_CSV_PATH):\n",
    "    print(f\"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {MESSAGES_CSV_PATH}\")\n",
    "else:\n",
    "    print(f\"âš ï¸  CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {MESSAGES_CSV_PATH}\")\n",
    "    print(\"ğŸ’¡ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€ãƒ‘ã‚¹ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„\")\n",
    "\n",
    "print(\"\\nğŸ“‹ è¨­å®šå†…å®¹:\")\n",
    "print(f\"   ãƒ¢ãƒ‡ãƒ«: {MODEL_NAME}\")\n",
    "print(f\"   Slug: {slug}\")\n",
    "print(f\"   æœ€å°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {MIN_MESSAGES}\")\n",
    "print(f\"   ãƒ¢ãƒ¼ãƒ‰: {mode}\")\n",
    "print(f\"   æ¸©åº¦: {TEMPERATURE}\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "config"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1: åˆå›ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆç·¨é›†å¯èƒ½ï¼‰\n",
    "\n",
    "INITIAL_PROMPT_TEMPLATE = \"\"\"ã‚ãªãŸã¯è¤‡æ•°ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’åˆ†æã—ã€æ¨ªæ–­çš„ãªè€ƒå¯Ÿã‚’è¡Œã†å°‚é–€å®¶ã§ã™ã€‚\n",
    "ä¸‹è¨˜ã®æƒ…å ±ã‚’å…ƒã«ã€ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã®ã¾ã¨ã‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "## ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã®èƒŒæ™¯\n",
    "\n",
    "ä»¥ä¸‹ã¯ã€Œ{{interviewTitle}}ã€ã¨ã„ã†ãƒ†ãƒ¼ãƒã§å®Ÿæ–½ã•ã‚ŒãŸã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã®åˆ†æä¾é ¼ã§ã™ã€‚\n",
    "\n",
    "ã€èª¬æ˜ã€‘\n",
    "{{interviewDescription}}\n",
    "\n",
    "ã€æ¦‚è¦ã€‘\n",
    "{{interviewOverview}}\n",
    "\n",
    "ã€ä¸»ãªãƒ†ãƒ¼ãƒã€‘\n",
    "{{interviewThemes}}\n",
    "\n",
    "ã€ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã§æ‰±ã£ãŸè³ªå•ã€‘\n",
    "{{interviewQuestions}}\n",
    "\n",
    "ã€å‚è€ƒçŸ¥è­˜ãƒ»ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘\n",
    "{{knowledgeContext}}\n",
    "\n",
    "ã“ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã¯ä¸Šè¨˜ã®èƒŒæ™¯ã€è³ªå•ã€å‚è€ƒçŸ¥è­˜ã«åŸºã¥ã„ã¦å®Ÿæ–½ã•ã‚Œã¾ã—ãŸã€‚ã“ã®æ–‡è„ˆã‚’è¸ã¾ãˆã¦ã€ä»¥ä¸‹ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## æ¤œè¨¼å¯¾è±¡ã®ä»®èª¬\n",
    "\n",
    "ä»¥ä¸‹ã®ã€Œäººå·¥çŸ¥èƒ½åŸºæœ¬è¨ˆç”»ã«å¯¾ã™ã‚‹æè¨€ã€ã«ãŠã‘ã‚‹ä¸»è¦ãªä»®èª¬ãƒ»ææ¡ˆã«ã¤ã„ã¦ã€ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ”¯æŒã™ã‚‹æ„è¦‹ã¨åè«–ãƒ»ç•°ãªã‚‹è¦–ç‚¹ã‚’æŠ½å‡ºã—ã€æ¤œè¨¼ã—ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "ã€ãƒãƒ¼ãƒ ã¿ã‚‰ã„ã®ææ¡ˆãƒ»ä»®èª¬ã€‘\n",
    "\n",
    "ä»®èª¬1: ã‚¢ã‚¸ãƒªãƒ†ã‚£ã‚’æ‹…ä¿ã™ã‚‹ä»•çµ„ã¿ä½œã‚Šã®å¿…è¦æ€§\n",
    "AIæŠ€è¡“ã¯3ãƒ¶æœˆã”ã¨ã«çŠ¶æ³ãŒå¤‰åŒ–ã™ã‚‹ãŸã‚ã€å¾“æ¥ã®å˜å¹´åº¦äºˆç®—ãƒ»è©³ç´°ä»•æ§˜å›ºå®šå‹ã®ITèª¿é”ã§ã¯æŠ€è¡“é©æ–°ã«å¯¾å¿œã§ããªã„ã€‚è¤‡æ•°å¹´åº¦äºˆç®—æ ã€3ãƒ¶æœˆã”ã¨ã®æŠ•è³‡é ˜åŸŸè¦‹ç›´ã—ã€ã€Œã‚¹ã‚­ãƒ£ãƒ³>ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆ>ã‚¹ã‚±ãƒ¼ãƒ«ã€æ–¹å¼ã®æ¡ç”¨ãŒå¿…è¦ã§ã‚ã‚‹ã€‚\n",
    "\n",
    "ä»®èª¬2: Japan AI Safety Instituteï¼ˆAISIï¼‰ã®è‚²æˆã®é‡è¦æ€§\n",
    "AIå®‰å…¨æ€§è©•ä¾¡ãƒ»æ¨™æº–åŒ–ãƒ»å…¬å…±èª¿é”è©•ä¾¡ã‚’æ‹…ã†AISIã‚’æ‹¡å¤§ã—ã€æ°‘é–“AIä¼æ¥­ç›¸å½“ã®å ±é…¬ä½“ç³»ã§å„ªç§€ãªäººæã‚’ç¢ºä¿ã™ã¹ãã§ã‚ã‚‹ã€‚AISIãŒæ”¿åºœã®AIåˆ©ç”¨ã€AIã‚³ãƒ¢ãƒ³ã‚ºæ•´å‚™ã€èªè¨¼åˆ¶åº¦ã®åŸºç¤ã‚’ä¸»å°ã™ã‚‹ã“ã¨ãŒæ—¥æœ¬ã®AIæˆ¦ç•¥ã«ä¸å¯æ¬ ã§ã‚ã‚‹ã€‚\n",
    "\n",
    "ä»®èª¬3: æ”¿åºœä¸»å°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•´å‚™ï¼ˆAIã‚³ãƒ¢ãƒ³ã‚ºï¼‰ã®å¿…è¦æ€§\n",
    "æ—¥æœ¬ç‰ˆã€ŒAIã‚³ãƒ¢ãƒ³ã‚ºã€ã‚’å‰µè¨­ã—ã€NHKç­‰ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ãƒ¼ã‚¿ã€è¡Œæ”¿ãƒ‡ãƒ¼ã‚¿ã€ç ”ç©¶æ©Ÿé–¢ãƒ‡ãƒ¼ã‚¿ã‚’æ©Ÿæ¢°å¯èª­å½¢å¼ã§æ•´å‚™ãƒ»å…¬é–‹ã—ã€å›½å†…AIäº‹æ¥­è€…ã«å„ªå…ˆåˆ©ç”¨ã•ã›ã‚‹ã“ã¨ã§å›½å†…AIé–‹ç™ºã‚’æ´»æ€§åŒ–ã§ãã‚‹ã€‚\n",
    "\n",
    "ä»®èª¬4: å›½å†…ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä¿è­·ã¨AIè¦åˆ¶ã®ãƒãƒ©ãƒ³ã‚¹\n",
    "AIäº‹æ¥­è€…ã«ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆãŒè‘—ä½œæ¨©ä¾µå®³ã—ãªã„ãŸã‚ã®æŠ€è¡“çš„ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«è¨­ç½®ã‚’èª²ã™ã¹ãã ãŒã€æµ·å¤–ã¨æ¯”ã¹éåº¦ãªè¦åˆ¶ã¯æ—¥æœ¬ã®AIäº‹æ¥­è€…ã®ç™ºå±•ã‚’å¦¨ã’ã‚‹ã€‚å›½éš›å‹•å‘ã¨æ•´åˆæ€§ã‚’å–ã‚Šã¤ã¤æ®µéšçš„ãƒ»æ¼¸é€²çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¿…è¦ã§ã‚ã‚‹ã€‚\n",
    "\n",
    "ä»®èª¬5: ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ•ã‚§ã‚¤ã‚¯å¯¾ç­–æŠ€è¡“ã¸ã®æŠ•è³‡ã®é‡è¦æ€§\n",
    "ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ•ã‚§ã‚¤ã‚¯åˆ¤å®šæŠ€è¡“ã€ç”Ÿæˆç”»åƒã®å®Ÿåœ¨åˆ¤å®šæŠ€è¡“ã¸ã®æŠ•è³‡ãŒã€æ‚ªç”¨å–ç· ã¨è¡¨ç¾ã®è‡ªç”±ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ä¸Šã§é‡è¦ã§ã‚ã‚‹ã€‚æŠ€è¡“çš„åˆ¤å®šæ‰‹æ®µãŒã‚ã‚Œã°å®Ÿæ…‹ã«å¿œã˜ãŸé‡åˆ‘åˆ¤æ–­ãŒå¯èƒ½ã«ãªã‚‹ã€‚\n",
    "\n",
    "ä»®èª¬6: è‡ªå‹•é‹è»¢é ˜åŸŸã¸ã®æ”¿åºœä¸»å°æ”¯æ´ã®å¿…è¦æ€§\n",
    "2027å¹´ã¾ã§ã«å…¨æ”¿ä»¤æŒ‡å®šéƒ½å¸‚ã§ç„¡äººè‡ªå‹•é‹è»¢è»Šã‚’èµ°ã‚‰ã›ã‚‹ç›®æ¨™ã‚’è¨­å®šã—ã€è¡Œæ”¿ãŒç©æ¥µçš„ã«è·¯è»Šå”èª¿ã‚·ã‚¹ãƒ†ãƒ å°å…¥ã‚„é“è·¯å†å»ºã‚’è¡Œã†ã“ã¨ã§ã€å›½å†…è‡ªå‹•é‹è»¢å¸‚å ´ã‚’æ´»æ€§åŒ–ã§ãã‚‹ã€‚å›½ç”£æŠ€è¡“é–‹ç™ºæ”¯æ´ã¨å›½éš›æ¨™æº–åŒ–ã¸ã®é–¢ä¸ãŒæ—¥æœ¬ã®ç«¶äº‰åŠ›ç¶­æŒã«ä¸å¯æ¬ ã§ã‚ã‚‹ã€‚\n",
    "\n",
    "ä»®èª¬7: å¹´é–“æŠ•è³‡è¦æ¨¡ãƒ»æ•°å€¤ç›®æ¨™è¨­å®šã®é‡è¦æ€§\n",
    "ãƒ‰ã‚¤ãƒ„ï¼ˆå¹´é–“ç´„2,840å„„å††ï¼‰ã‚’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«æ—¥æœ¬ã®AIæŠ•è³‡ã‚’50%å¢—åŠ ã•ã›ã€è¨ˆç®—è³‡æºï¼ˆGPUè¦æ¨¡ï¼‰ã€AIäººæè‚²æˆï¼ˆæ–°è¦åšå£«å·å–å¾—è€…æ•°ã€AIæ•™æˆè·æ•°ï¼‰ã®å®šé‡ç›®æ¨™ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã€çœåºæ¨ªæ–­ã®ç›®æ¨™ä¸€è‡´ã¨äº‹å¾Œæ¤œè¨¼ãŒå¯èƒ½ã«ãªã‚‹ã€‚\n",
    "\n",
    "\n",
    "## ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯\n",
    "\n",
    "ä¸‹è¨˜ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆ{{sessionCount}}ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰ã‚’æ¨ªæ–­çš„ã«åˆ†æã—ã€ä¸Šè¨˜ã®ä»®èª¬ã”ã¨ã«æ”¯æŒã™ã‚‹æ„è¦‹ã¨åè«–ãƒ»ç•°ãªã‚‹è¦–ç‚¹ã‚’æŠ½å‡ºã—ã¦Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "æ³¨æ„: ã“ã‚Œã¯åˆå›ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã§ã™ã€‚å¾Œã»ã©è¿½åŠ ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã™ã‚‹äºˆå®šãªã®ã§ã€ç¾æ™‚ç‚¹ã§ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ãŸåˆ†æã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "## å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\n",
    "\n",
    "# {{interviewTitle}} - é›†ç´„ãƒ¬ãƒãƒ¼ãƒˆ\n",
    "\n",
    "## ã¾ã¨ã‚\n",
    "\n",
    "[å…¨ä½“ã‚’é€šã˜ã¦è¦‹ãˆã¦ããŸä¸»è¦ãªãƒ†ãƒ¼ãƒã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã€é‡è¦ãªç™ºè¦‹ã‚’2æ®µè½ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚]\n",
    "\n",
    "## ä»®èª¬1: [äº‹å‰ã«è¨­å®šã—ãŸä»®èª¬]\n",
    "\n",
    "### æ”¯æŒã™ã‚‹æ„è¦‹\n",
    "- [æ„è¦‹1]\n",
    "- [æ„è¦‹2]\n",
    "\n",
    "### åè«–ãƒ»ç•°ãªã‚‹è¦–ç‚¹\n",
    "- [åè«–1]\n",
    "- [åè«–2]\n",
    "\n",
    "### æ¤œè¨¼çµæœ\n",
    "[ã“ã®ä»®èª¬ã«å¯¾ã™ã‚‹ç·åˆçš„ãªè©•ä¾¡]\n",
    "\n",
    "## ä»®èª¬2: [äº‹å‰ã«è¨­å®šã—ãŸä»®èª¬]\n",
    "\n",
    "[åŒæ§˜ã«ã€äº‹å‰ã«è¨­å®šã—ãŸä»®èª¬ã”ã¨ã«ç« ç«‹ã¦ã—ã¦æ¤œè¨¼çµæœã‚’è¨˜è¿°ã—ã¦ãã ã•ã„]\n",
    "\n",
    "---\n",
    "\n",
    "é‡è¦ãªæ³¨æ„äº‹é …:\n",
    "1. Markdownãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„\n",
    "2. HTMLã‚¿ã‚°ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„\n",
    "3. å¤ªå­—ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ï¼ˆã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯2ã¤ï¼‰ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚æ—¥æœ¬èªã§ã¯æ‹¬å¼§ã®å‡¦ç†ãŒã†ã¾ãã„ã‹ãªã„ã“ã¨ãŒå¤šã„ãŸã‚ã§ã™\n",
    "4. å‰ç½®ãã‚„èª¬æ˜ã¯ä¸è¦ã§ã€ã€Œ# {{interviewTitle}} - é›†ç´„ãƒ¬ãƒãƒ¼ãƒˆã€ã‹ã‚‰å§‹ã‚ã¦ãã ã•ã„\n",
    "5. å…·ä½“çš„ãªç™ºè¨€ã‚’å¼•ç”¨ã™ã‚‹å ´åˆã¯ã€å¿…ãšã€Œ#ã€ã‚’ä»˜ã‘ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ç•ªå·ã§å‡ºå…¸ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„\n",
    "6. å€‹ã€…ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¦ç´„ã§ã¯ãªãã€æ¨ªæ–­çš„ãªåˆ†æã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã ã•ã„\n",
    "7. ã€é‡è¦ã€‘ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼å‚åŠ è€…ã®ç”Ÿã®å£°ã‚’ç©æ¥µçš„ã«å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚ã€Œ\\\"ç™ºè¨€å†…å®¹\\\"(#ã‚»ãƒƒã‚·ãƒ§ãƒ³ç•ªå·)ã€ã®å½¢å¼ã§å¤šæ•°å¼•ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒªã‚¢ãƒªãƒ†ã‚£ã¨èª¬å¾—åŠ›ã®ã‚ã‚‹ãƒ¬ãƒãƒ¼ãƒˆã«ã—ã¦ãã ã•ã„\n",
    "8. ã€é‡è¦ã€‘ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼å¯¾è±¡è€…ã®æ•°é‡çš„æƒ…å ±ã‚„é‡çš„è¡¨ç¾ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚ä»¥ä¸‹ã®ã‚ˆã†ãªè¡¨ç¾ã¯ä½¿ç”¨ç¦æ­¢ã§ã™ï¼š\n",
    "   - ç¦æ­¢ä¾‹: ã€Œå¤šãã®å‚åŠ è€…ãŒã€ã€Œä¸€éƒ¨ã®äººãŒã€ã€Œã»ã¨ã‚“ã©ã®äººãŒã€ã€Œå¤§åŠãŒã€ã€Œå°‘æ•°ã ãŒã€\n",
    "   - æ¨å¥¨: ã€Œã‚ã‚‹å‚åŠ è€…ã¯ã€ã€Œã€œã¨ã„ã†æ„è¦‹ãŒã‚ã‚‹ã€ã€Œã€œã¨ã„ã†è¦–ç‚¹ã‚‚ç¤ºã•ã‚ŒãŸã€ãªã©\n",
    "9. ã€é‡è¦ã€‘ä¾¡å€¤åˆ¤æ–­ã‚„è©•ä¾¡ã‚’å«ã‚€è¡¨ç¾ã‚’é¿ã‘ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®ã‚ˆã†ãªè¡¨ç¾ã¯ä½¿ç”¨ç¦æ­¢ã§ã™ï¼š\n",
    "   - ç¦æ­¢ä¾‹: ã€Œã•ã‚‰ã«è¸ã¿è¾¼ã¿ã€ã€Œã‚ˆã‚Šé«˜åº¦ãªã€ã€Œæ·±ã„æ´å¯Ÿã€ã€Œå„ªã‚ŒãŸææ¡ˆã€ã€Œé‡è¦åº¦ãŒé«˜ã„ã€\n",
    "   - æ¨å¥¨: ã€Œã€œã¨ã„ã†æ„è¦‹ãŒã‚ã‚‹ã€ã€Œã€œã¨ã„ã†è¦–ç‚¹ã‚‚ç¤ºã•ã‚ŒãŸã€ãªã©ã€ä¸­ç«‹çš„ãªè¨˜è¿°ã‚’ä½¿ç”¨\n",
    "10. ã€é‡è¦ã€‘ãƒ¬ãƒˆãƒªãƒƒã‚¯ã‚„ä¸»è¦³çš„ãªä¿®é£¾èªã‚’æ’é™¤ã—ã€å­¦è¡“è«–æ–‡ã®ã‚ˆã†ã«å®¢è¦³çš„ã‹ã¤ç°¡æ½”ã«è¨˜è¿°ã—ã¦ãã ã•ã„\n",
    "11. ã€é‡è¦ã€‘è¡¨é¢çš„ãªè­°è«–ã«ã¨ã©ã¾ã‚‰ãªã„ã‚ˆã†ã«ã€æ„è¦‹ã«å¯¾ã™ã‚‹åè«–ã‚„å†åè«–ãªã©ã‚‚ç©æ¥µçš„ã«æ§‹é€ åŒ–ã—ãªãŒã‚‰å–ã‚Šä¸Šã’ã¦ãã ã•ã„\n",
    "12. {{outputLengthGuidance}}\n",
    "\n",
    "---\n",
    "\n",
    "## ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ ({{sessionCount}}ã‚»ãƒƒã‚·ãƒ§ãƒ³)\n",
    "\n",
    "{{sessionsData}}\n",
    "\"\"\"\n",
    "\n",
    "print(f\"âœ… INITIAL_PROMPT_TEMPLATE èª­ã¿è¾¼ã¿å®Œäº† ({len(INITIAL_PROMPT_TEMPLATE):,} æ–‡å­—)\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "prompt1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ2: æ›´æ–°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆç·¨é›†å¯èƒ½ï¼‰\n",
    "\n",
    "UPDATE_PROMPT_TEMPLATE = \"\"\"ã‚ãªãŸã¯è¤‡æ•°ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’åˆ†æã—ã€æ¨ªæ–­çš„ãªè€ƒå¯Ÿã‚’è¡Œã†å°‚é–€å®¶ã§ã™ã€‚\n",
    "\n",
    "## ã‚¿ã‚¹ã‚¯æ¦‚è¦\n",
    "\n",
    "ä»¥å‰ã«ä½œæˆã—ãŸé›†ç´„ãƒ¬ãƒãƒ¼ãƒˆã«ã€**è¿½åŠ ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼ˆ{{newSessionCount}}ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰ã‚’çµ±åˆ**ã—ã¦ã€ãƒ¬ãƒãƒ¼ãƒˆã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "**é‡è¦**: æœ€çµ‚çš„ãªãƒ¬ãƒãƒ¼ãƒˆã«ã¯ã€ã©ã®ãƒ‡ãƒ¼ã‚¿ãŒã€Œæ–°ã—ãè¿½åŠ ã•ã‚ŒãŸã€ã‹ã€ã€Œå‰å›ã®ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰ã€ã¨ã„ã£ãŸè¨˜è¿°ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚ã™ã¹ã¦ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä¸€åº¦ã«åˆ†æã—ãŸã‹ã®ã‚ˆã†ã«ã€ä¸€ã¤ã®çµ±åˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "## ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã®èƒŒæ™¯\n",
    "\n",
    "ä»¥ä¸‹ã¯ã€Œ{{interviewTitle}}ã€ã¨ã„ã†ãƒ†ãƒ¼ãƒã§å®Ÿæ–½ã•ã‚ŒãŸã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã®åˆ†æã§ã™ã€‚\n",
    "\n",
    "ã€èª¬æ˜ã€‘\n",
    "{{interviewDescription}}\n",
    "\n",
    "ã€æ¦‚è¦ã€‘\n",
    "{{interviewOverview}}\n",
    "\n",
    "ã€ä¸»ãªãƒ†ãƒ¼ãƒã€‘\n",
    "{{interviewThemes}}\n",
    "\n",
    "ã€ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã§æ‰±ã£ãŸè³ªå•ã€‘\n",
    "{{interviewQuestions}}\n",
    "\n",
    "ã€å‚è€ƒçŸ¥è­˜ãƒ»ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘\n",
    "{{knowledgeContext}}\n",
    "\n",
    "---\n",
    "\n",
    "## å‰å›ã®ãƒ¬ãƒãƒ¼ãƒˆ\n",
    "\n",
    "ä»¥ä¸‹ã¯ã€ã“ã‚Œã¾ã§ã«åˆ†æã—ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰ä½œæˆã—ãŸãƒ¬ãƒãƒ¼ãƒˆã§ã™ï¼š\n",
    "\n",
    "{{previousReport}}\n",
    "\n",
    "---\n",
    "\n",
    "## æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ ({{newSessionCount}}ã‚»ãƒƒã‚·ãƒ§ãƒ³)\n",
    "\n",
    "ä»¥ä¸‹ã®æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’åˆ†æã—ã€ä¸Šè¨˜ã®ãƒ¬ãƒãƒ¼ãƒˆã«çµ±åˆã—ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "{{sessionsData}}\n",
    "\n",
    "---\n",
    "\n",
    "## ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯\n",
    "\n",
    "1. è¿½åŠ ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€å‰å›ã®ãƒ¬ãƒãƒ¼ãƒˆã«å«ã¾ã‚Œã¦ã„ãªã„ç™ºè¦‹ã€è¦–ç‚¹ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®šã—ã¦ãã ã•ã„\n",
    "2. å‰å›ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’æ›´æ–°ã—ã¦ãã ã•ã„ï¼š\n",
    "   - æ–°ã—ã„ç™ºè¦‹ãŒã‚ã‚Œã°ã€è©²å½“ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã«è¿½åŠ \n",
    "   - æ–°ã—ã„ã‚«ãƒ†ã‚´ãƒªãŒå¿…è¦ã§ã‚ã‚Œã°è¿½åŠ \n",
    "   - æ—¢å­˜ã®è¨˜è¿°ãŒè¿½åŠ ãƒ‡ãƒ¼ã‚¿ã§è£œå¼·ã•ã‚Œã‚‹å ´åˆã¯ã€å¼•ç”¨ã‚’è¿½åŠ \n",
    "   - ã€Œã¾ã¨ã‚ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚‚è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚’åæ˜ ã—ã¦æ›´æ–°\n",
    "3. å‰å›ã®ãƒ¬ãƒãƒ¼ãƒˆã®è‰¯ã„éƒ¨åˆ†ã¯ç¶­æŒã—ã¦ãã ã•ã„ï¼ˆä¸è¦ãªå‰Šé™¤ã‚„æ›¸ãæ›ãˆã¯é¿ã‘ã‚‹ï¼‰\n",
    "\n",
    "**çµ¶å¯¾ã«å®ˆã‚‹ã¹ããƒ«ãƒ¼ãƒ«:**\n",
    "- ã€Œæ–°ãŸãªã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚Šã€ã€Œè¿½åŠ ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ã€Œå‰å›ã®ãƒ¬ãƒãƒ¼ãƒˆã§ã¯ã€ã¨ã„ã£ãŸã€åˆ†æãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¤ºå”†ã™ã‚‹è¡¨ç¾ã¯ä¸€åˆ‡ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„\n",
    "- ã€Œã‚ˆã‚Šå…·ä½“çš„ãªã€ã€Œã•ã‚‰ã«è©³ç´°ãªã€ãªã©ã€ãƒ¬ãƒãƒ¼ãƒˆã®æ›´æ–°å±¥æ­´ã‚’ç¤ºã™è¡¨ç¾ã‚‚é¿ã‘ã¦ãã ã•ã„\n",
    "- ã™ã¹ã¦ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æœ€åˆã‹ã‚‰ä¸€åº¦ã«åˆ†æã—ãŸã‹ã®ã‚ˆã†ã«è¨˜è¿°ã—ã¦ãã ã•ã„\n",
    "- å¼•ç”¨ã‚’è¿½åŠ ã™ã‚‹å ´åˆã‚‚ã€å¿…ãšã€Œ#ã€ã‚’ä»˜ã‘ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ç•ªå·ã§å‡ºå…¸ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„\n",
    "- å‰å›ã®ãƒ¬ãƒãƒ¼ãƒˆã®æ§‹é€ ã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ç¶­æŒã—ã¦ãã ã•ã„\n",
    "- å‰ç½®ãã‚„èª¬æ˜ã¯ä¸è¦ã§ã€æ›´æ–°ã•ã‚ŒãŸã€Œ# {{interviewTitle}} - é›†ç´„ãƒ¬ãƒãƒ¼ãƒˆã€ã‚’ç›´æ¥å‡ºåŠ›ã—ã¦ãã ã•ã„\n",
    "\n",
    "æ³¨æ„äº‹é …:\n",
    "1. Markdownãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„\n",
    "2. HTMLã‚¿ã‚°ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„\n",
    "3. å¤ªå­—ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ï¼ˆã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯2ã¤ï¼‰ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚æ—¥æœ¬èªã§ã¯æ‹¬å¼§ã®å‡¦ç†ãŒã†ã¾ãã„ã‹ãªã„ã“ã¨ãŒå¤šã„ãŸã‚ã§ã™\n",
    "4. å…·ä½“çš„ãªç™ºè¨€ã‚’å¼•ç”¨ã™ã‚‹å ´åˆã¯ã€å¿…ãšã€Œ#ã€ã‚’ä»˜ã‘ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ç•ªå·ã§å‡ºå…¸ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„\n",
    "5. ã€é‡è¦ã€‘ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼å‚åŠ è€…ã®ç”Ÿã®å£°ã‚’ç©æ¥µçš„ã«å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚ã€Œ\\\"ç™ºè¨€å†…å®¹\\\"(#ã‚»ãƒƒã‚·ãƒ§ãƒ³ç•ªå·)ã€ã®å½¢å¼ã§å¼•ç”¨ã—ã¦ãã ã•ã„\n",
    "6. ã€é‡è¦ã€‘æ•°é‡çš„æƒ…å ±ã‚„é‡çš„è¡¨ç¾ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ (ã€Œå¤šãã®å‚åŠ è€…ãŒã€ã€Œä¸€éƒ¨ã®äººãŒã€ãªã©ç¦æ­¢)\n",
    "7. ã€é‡è¦ã€‘ä¾¡å€¤åˆ¤æ–­ã‚„è©•ä¾¡ã‚’å«ã‚€è¡¨ç¾ã‚’é¿ã‘ã¦ãã ã•ã„ (ã€Œæ·±ã„æ´å¯Ÿã€ã€Œå„ªã‚ŒãŸææ¡ˆã€ãªã©ç¦æ­¢)\n",
    "8. ã€é‡è¦ã€‘ãƒ¬ãƒˆãƒªãƒƒã‚¯ã‚„ä¸»è¦³çš„ãªä¿®é£¾èªã‚’æ’é™¤ã—ã€å­¦è¡“è«–æ–‡ã®ã‚ˆã†ã«å®¢è¦³çš„ã‹ã¤ç°¡æ½”ã«è¨˜è¿°ã—ã¦ãã ã•ã„\n",
    "9. {{outputLengthGuidance}}\n",
    "\n",
    "---\n",
    "\n",
    "æ›´æ–°ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š\n",
    "\"\"\"\n",
    "\n",
    "print(f\"âœ… UPDATE_PROMPT_TEMPLATE èª­ã¿è¾¼ã¿å®Œäº† ({len(UPDATE_PROMPT_TEMPLATE):,} æ–‡å­—)\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "prompt2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ3: ãƒãƒ¼ã‚¸ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆç·¨é›†å¯èƒ½ï¼‰\n",
    "\n",
    "MERGE_PROMPT_TEMPLATE = \"\"\"ã‚ãªãŸã¯è¤‡æ•°ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’åˆ†æã—ã€æ¨ªæ–­çš„ãªè€ƒå¯Ÿã‚’è¡Œã†å°‚é–€å®¶ã§ã™ã€‚\n",
    "\n",
    "## ã‚¿ã‚¹ã‚¯æ¦‚è¦\n",
    "\n",
    "è¤‡æ•°ã®ãƒãƒƒãƒã«åˆ†ã‘ã¦ä¸¦åˆ—ã«åˆ†æã•ã‚ŒãŸã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ã€**ä¸€ã¤ã®çµ±åˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ**ã«ãƒãƒ¼ã‚¸ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "## ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã®èƒŒæ™¯\n",
    "\n",
    "ä»¥ä¸‹ã¯ã€Œ{{interviewTitle}}ã€ã¨ã„ã†ãƒ†ãƒ¼ãƒã§å®Ÿæ–½ã•ã‚ŒãŸã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã®åˆ†æã§ã™ã€‚\n",
    "\n",
    "ã€èª¬æ˜ã€‘\n",
    "{{interviewDescription}}\n",
    "\n",
    "ã€æ¦‚è¦ã€‘\n",
    "{{interviewOverview}}\n",
    "\n",
    "ã€ä¸»ãªãƒ†ãƒ¼ãƒã€‘\n",
    "{{interviewThemes}}\n",
    "\n",
    "ã€ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã§æ‰±ã£ãŸè³ªå•ã€‘\n",
    "{{interviewQuestions}}\n",
    "\n",
    "ã€å‚è€ƒçŸ¥è­˜ãƒ»ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘\n",
    "{{knowledgeContext}}\n",
    "\n",
    "---\n",
    "\n",
    "## ãƒãƒƒãƒãƒ¬ãƒãƒ¼ãƒˆ ({{batchCount}}å€‹ã®ãƒ¬ãƒãƒ¼ãƒˆ)\n",
    "\n",
    "ä»¥ä¸‹ã¯ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’{{batchCount}}å€‹ã®ãƒãƒƒãƒã«åˆ†ã‘ã¦ä¸¦åˆ—ã«åˆ†æã—ãŸçµæœã§ã™ï¼š\n",
    "\n",
    "{{batchReports}}\n",
    "\n",
    "---\n",
    "\n",
    "## ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯\n",
    "\n",
    "ä¸Šè¨˜ã®{{batchCount}}å€‹ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’çµ±åˆã—ã€ä¸€ã¤ã®åŒ…æ‹¬çš„ãªé›†ç´„ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "**é‡è¦ãªæ³¨æ„äº‹é …:**\n",
    "1. **æƒ…å ±ã®å®Œå…¨ä¿æŒ**: ãƒãƒ¼ã‚¸å‰ã®ãƒ¬ãƒãƒ¼ãƒˆã«ã‚ã£ãŸæƒ…å ±ã‚’å¤±ã‚ãªã„ã§ãã ã•ã„ã€‚ã™ã¹ã¦ã®é‡è¦ãªç™ºè¦‹ã€æŒ‡æ‘˜ã€å¼•ç”¨ã‚’çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã«å«ã‚ã¦ãã ã•ã„\n",
    "2. **ç”Ÿã®å¼•ç”¨ã‚’ãã®ã¾ã¾å¼•ãç¶™ã**: å„ãƒãƒƒãƒãƒ¬ãƒãƒ¼ãƒˆã«å«ã¾ã‚Œã‚‹å…·ä½“çš„ãªç™ºè¨€ã®å¼•ç”¨ã¯ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ç•ªå·ã¨ã¨ã‚‚ã«ãã®ã¾ã¾çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã«å«ã‚ã¦ãã ã•ã„\n",
    "3. **é‡è¤‡ã®æ’é™¤**: è¤‡æ•°ã®ãƒãƒƒãƒã§åŒã˜ç™ºè¦‹ã‚„æ„è¦‹ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿ã€ä¸€åº¦ã ã‘è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚ãŸã ã—å¼•ç”¨ã¯ç•°ãªã‚‹ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰ã®ã‚‚ã®ã§ã‚ã‚Œã°è¤‡æ•°æ®‹ã—ã¦ãã ã•ã„\n",
    "4. **ã‚«ãƒ†ã‚´ãƒªã®å†æ§‹æˆ**: ãƒãƒƒãƒã”ã¨ã®ã‚«ãƒ†ã‚´ãƒªã‚’è¦‹ç›´ã—ã€ã‚ˆã‚Šé©åˆ‡ãªåˆ†é¡ã«å†æ§‹æˆã—ã¦ãã ã•ã„\n",
    "5. **ç¶²ç¾…æ€§ã®ç¢ºä¿**: ã™ã¹ã¦ã®ãƒãƒƒãƒã‹ã‚‰é‡è¦ãªç™ºè¦‹ã‚’æ¼ã‚‰ã•ãšå«ã‚ã¦ãã ã•ã„ã€‚ãƒãƒ¼ã‚¸å‰ã®ãƒ¬ãƒãƒ¼ãƒˆã§è¨€åŠã•ã‚Œã¦ã„ãŸè«–ç‚¹ã‚„è¦–ç‚¹ãŒã™ã¹ã¦çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã«åæ˜ ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„\n",
    "6. **ä¸€è²«æ€§ã®ç¶­æŒ**: ãƒˆãƒ¼ãƒ³ã‚„ã‚¹ã‚¿ã‚¤ãƒ«ã‚’çµ±ä¸€ã—ã€ä¸€ã¤ã®ãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦è‡ªç„¶ã«èª­ã‚ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„\n",
    "7. **æ–‡å­—æ•°ã«ã¤ã„ã¦**: æƒ…å ±ã‚’ä¿æŒã™ã‚‹ãŸã‚ã«æ–‡å­—æ•°ãŒå¢—ãˆã‚‹ã“ã¨ã¯å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚åŒ…æ‹¬çš„ã§è©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„\n",
    "\n",
    "**çµ¶å¯¾ã«å®ˆã‚‹ã¹ããƒ«ãƒ¼ãƒ«:**\n",
    "- ã€Œãƒãƒƒãƒ1ã§ã¯ã€ã€Œãƒãƒƒãƒ2ã®åˆ†æã«ã‚ˆã‚‹ã¨ã€ã¨ã„ã£ãŸã€ãƒãƒƒãƒåˆ†å‰²ã‚’ç¤ºå”†ã™ã‚‹è¡¨ç¾ã¯ä¸€åˆ‡ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„\n",
    "- ã™ã¹ã¦ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æœ€åˆã‹ã‚‰ä¸€åº¦ã«åˆ†æã—ãŸã‹ã®ã‚ˆã†ã«è¨˜è¿°ã—ã¦ãã ã•ã„\n",
    "- å¼•ç”¨ã™ã‚‹å ´åˆã‚‚ã€å¿…ãšã€Œ#ã€ã‚’ä»˜ã‘ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ç•ªå·ã§å‡ºå…¸ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„\n",
    "- Markdownãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„\n",
    "- HTMLã‚¿ã‚°ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„\n",
    "- å¤ªå­—ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ï¼ˆã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯2ã¤ï¼‰ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„\n",
    "- å‰ç½®ãã‚„èª¬æ˜ã¯ä¸è¦ã§ã€ã€Œ# {{interviewTitle}} - é›†ç´„ãƒ¬ãƒãƒ¼ãƒˆã€ã‹ã‚‰å§‹ã‚ã¦ãã ã•ã„\n",
    "- æ•°é‡çš„æƒ…å ±ã‚„é‡çš„è¡¨ç¾ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ (ã€Œå¤šãã®å‚åŠ è€…ãŒã€ãªã©ç¦æ­¢)\n",
    "- ä¾¡å€¤åˆ¤æ–­ã‚„è©•ä¾¡ã‚’å«ã‚€è¡¨ç¾ã‚’é¿ã‘ã¦ãã ã•ã„ (ã€Œæ·±ã„æ´å¯Ÿã€ãªã©ç¦æ­¢)\n",
    "- ãƒ¬ãƒˆãƒªãƒƒã‚¯ã‚„ä¸»è¦³çš„ãªä¿®é£¾èªã‚’æ’é™¤ã—ã€å­¦è¡“è«–æ–‡ã®ã‚ˆã†ã«å®¢è¦³çš„ã‹ã¤ç°¡æ½”ã«è¨˜è¿°ã—ã¦ãã ã•ã„\n",
    "- {{outputLengthGuidance}}\n",
    "\n",
    "## å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\n",
    "\n",
    "# {{interviewTitle}} - é›†ç´„ãƒ¬ãƒãƒ¼ãƒˆ\n",
    "\n",
    "## ã¾ã¨ã‚\n",
    "\n",
    "[å…¨ä½“ã‚’é€šã˜ã¦è¦‹ãˆã¦ããŸä¸»è¦ãªãƒ†ãƒ¼ãƒã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã€é‡è¦ãªç™ºè¦‹ã‚’2æ®µè½ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚]\n",
    "\n",
    "## ä»®èª¬1: [äº‹å‰ã«è¨­å®šã—ãŸä»®èª¬]\n",
    "\n",
    "### æ”¯æŒã™ã‚‹æ„è¦‹\n",
    "- [æ„è¦‹1]\n",
    "- [æ„è¦‹2]\n",
    "\n",
    "### åè«–ãƒ»ç•°ãªã‚‹è¦–ç‚¹\n",
    "- [åè«–1]\n",
    "- [åè«–2]\n",
    "\n",
    "### æ¤œè¨¼çµæœ\n",
    "[ã“ã®ä»®èª¬ã«å¯¾ã™ã‚‹ç·åˆçš„ãªè©•ä¾¡]\n",
    "\n",
    "## ä»®èª¬2: [äº‹å‰ã«è¨­å®šã—ãŸä»®èª¬]\n",
    "\n",
    "[åŒæ§˜ã«ã€äº‹å‰ã«è¨­å®šã—ãŸä»®èª¬ã”ã¨ã«ç« ç«‹ã¦ã—ã¦æ¤œè¨¼çµæœã‚’è¨˜è¿°ã—ã¦ãã ã•ã„]\n",
    "\n",
    "---\n",
    "\n",
    "çµ±åˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š\n",
    "\"\"\"\n",
    "\n",
    "print(f\"âœ… MERGE_PROMPT_TEMPLATE èª­ã¿è¾¼ã¿å®Œäº† ({len(MERGE_PROMPT_TEMPLATE):,} æ–‡å­—)\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "prompt3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title ğŸ› ï¸ ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°å®šç¾©\n",
    "\n",
    "def configure_genai(api_key: str):\n",
    "    \"\"\"Google Generative AI ã®è¨­å®š\"\"\"\n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"âœ… Google Generative AI ã®è¨­å®šå®Œäº†\")\n",
    "\n",
    "def generate_text_with_gemini(\n",
    "    model_name: str,\n",
    "    prompt: str,\n",
    "    temperature: float = 0.3,\n",
    "    max_output_tokens: int = 64000,\n",
    "    max_retries: int = 3,\n",
    "    retry_delay: int = 5 # seconds\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Google Generative AI SDK ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            \"text\": str,  # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ\n",
    "            \"usage\": {    # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡\n",
    "                \"prompt_tokens\": int,\n",
    "                \"completion_tokens\": int,\n",
    "                \"total_tokens\": int\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    \n",
    "    generation_config = genai.types.GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_output_tokens,\n",
    "    )\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=generation_config\n",
    "            )\n",
    "            \n",
    "            # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®å–å¾—\n",
    "            usage = {\n",
    "                \"prompt_tokens\": response.usage_metadata.prompt_token_count,\n",
    "                \"completion_tokens\": response.usage_metadata.candidates_token_count,\n",
    "                \"total_tokens\": response.usage_metadata.total_token_count\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                \"text\": response.text,\n",
    "                \"usage\": usage\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ APIå‘¼ã³å‡ºã—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ (è©¦è¡Œ {attempt + 1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"   {retry_delay}ç§’å¾Œã«å†è©¦è¡Œã—ã¾ã™...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(\"âŒ æœ€å¤§å†è©¦è¡Œå›æ•°ã«é”ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚\")\n",
    "                raise e\n",
    "\n",
    "def load_messages_csv(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"messages.csv ã‚’èª­ã¿è¾¼ã¿\"\"\"\n",
    "    print(f\"ğŸ“‚ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"âœ… {len(df):,} ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\")\n",
    "    print(f\"ğŸ“Š ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {df['session_id'].nunique():,}\")\n",
    "    return df\n",
    "\n",
    "def filter_sessions(\n",
    "    df: pd.DataFrame,\n",
    "    slug: str,\n",
    "    min_messages: int = 7\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    æ¡ä»¶ã«åˆã†ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã®ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
    "    \n",
    "    Args:\n",
    "        df: messages DataFrame\n",
    "        slug: ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã®slug\n",
    "        min_messages: æœ€å°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°\n",
    "    \n",
    "    Returns:\n",
    "        session_idã®ãƒªã‚¹ãƒˆï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ã®é™é †ï¼‰\n",
    "    \"\"\"\n",
    "    # slugã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆsession_idã«slugãŒå«ã¾ã‚Œã‚‹ã‚‚ã®ï¼‰\n",
    "    filtered = df[df['session_id'].str.contains(slug, na=False, regex=False)]\n",
    "    \n",
    "    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã”ã¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    session_counts = filtered.groupby('session_id').size()\n",
    "    \n",
    "    # æœ€å°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ã§ãƒ•ã‚£ãƒ«ã‚¿\n",
    "    valid_sessions = session_counts[session_counts >= min_messages]\n",
    "    \n",
    "    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ã®é™é †ã§ã‚½ãƒ¼ãƒˆ\n",
    "    sorted_sessions = valid_sessions.sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ:\")\n",
    "    print(f\"   ç·ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {len(session_counts):,}\")\n",
    "    print(f\"   æœ‰åŠ¹ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ï¼ˆ{min_messages}ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä»¥ä¸Šï¼‰: {len(valid_sessions):,}\")\n",
    "    print(f\"   é™¤å¤–ã•ã‚ŒãŸã‚»ãƒƒã‚·ãƒ§ãƒ³: {len(session_counts) - len(valid_sessions):,}\")\n",
    "    \n",
    "    return sorted_sessions.index.tolist()\n",
    "\n",
    "def get_session_data(df: pd.DataFrame, session_ids: List[str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    æŒ‡å®šã•ã‚ŒãŸsession_idã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—\n",
    "    \n",
    "    Returns:\n",
    "        [\n",
    "            {\n",
    "                \"id\": \"session-1\",\n",
    "                \"sessionNumber\": 1,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"...\", \"timestamp\": \"...\"},\n",
    "                    ...\n",
    "                ]\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "    sessions_data = []\n",
    "    \n",
    "    for idx, session_id in enumerate(session_ids):\n",
    "        session_messages = df[df['session_id'] == session_id].sort_values('timestamp')\n",
    "        \n",
    "        messages = []\n",
    "        for _, row in session_messages.iterrows():\n",
    "            messages.append({\n",
    "                \"role\": row['role'],\n",
    "                \"content\": str(row['content']),\n",
    "                \"timestamp\": str(row['timestamp'])\n",
    "            })\n",
    "        \n",
    "        if len(messages) > 0:\n",
    "            sessions_data.append({\n",
    "                \"id\": session_id,\n",
    "                \"sessionNumber\": idx + 1,\n",
    "                \"messages\": messages\n",
    "            })\n",
    "    \n",
    "    return sessions_data\n",
    "\n",
    "def format_sessions_data(sessions: List[Dict]) -> str:\n",
    "    \"\"\"ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’Markdownå½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\"\"\"\n",
    "    formatted = []\n",
    "    \n",
    "    for session in sessions:\n",
    "        messages_text = \"\\n\\n\".join([\n",
    "            f\"**{msg['role']}**: {msg['content']}\"\n",
    "            for msg in session['messages']\n",
    "        ])\n",
    "        \n",
    "        formatted.append(\n",
    "            f\"### Session #{session['sessionNumber']}\\n\\n{messages_text}\"\n",
    "        )\n",
    "    \n",
    "    return \"\\n\\n---\\n\\n\".join(formatted)\n",
    "\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æ¦‚ç®—ï¼ˆ1ãƒˆãƒ¼ã‚¯ãƒ³ â‰ˆ 4æ–‡å­—ï¼‰\"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "def create_batches(\n",
    "    session_ids: List[str],\n",
    "    message_counts: Dict[str, int],\n",
    "    tokens_per_batch: int,\n",
    "    prompt_overhead: int\n",
    ") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒãƒƒãƒã«åˆ†å‰²\n",
    "    \n",
    "    Args:\n",
    "        session_ids: å…¨ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã®ãƒªã‚¹ãƒˆï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°é™é †ï¼‰\n",
    "        message_counts: {session_id: message_count}\n",
    "        tokens_per_batch: ãƒãƒƒãƒã‚ãŸã‚Šã®ç›®æ¨™ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "        prompt_overhead: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰\n",
    "    \n",
    "    Returns:\n",
    "        ãƒãƒƒãƒã®ãƒªã‚¹ãƒˆ [[session_id, ...], ...]\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_tokens = prompt_overhead\n",
    "    \n",
    "    for session_id in session_ids:\n",
    "        message_count = message_counts.get(session_id, 0)\n",
    "        estimated_session_tokens = message_count * 350  # 1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚ãŸã‚Šç´„350ãƒˆãƒ¼ã‚¯ãƒ³\n",
    "        \n",
    "        if current_tokens + estimated_session_tokens > tokens_per_batch and current_batch:\n",
    "            # ç¾åœ¨ã®ãƒãƒƒãƒã‚’ç¢ºå®šã—ã€æ–°ã—ã„ãƒãƒƒãƒã‚’é–‹å§‹\n",
    "            batches.append(current_batch)\n",
    "            current_batch = [session_id]\n",
    "            current_tokens = prompt_overhead + estimated_session_tokens\n",
    "        else:\n",
    "            current_batch.append(session_id)\n",
    "            current_tokens += estimated_session_tokens\n",
    "    \n",
    "    if current_batch:\n",
    "        batches.append(current_batch)\n",
    "    \n",
    "    print(f\"\\nğŸ“¦ ãƒãƒƒãƒåˆ†å‰²çµæœ:\")\n",
    "    print(f\"   ç·ãƒãƒƒãƒæ•°: {len(batches)}\")\n",
    "    for i, batch in enumerate(batches):\n",
    "        print(f\"   ãƒãƒƒãƒ {i+1}: {len(batch)} ã‚»ãƒƒã‚·ãƒ§ãƒ³\")\n",
    "    \n",
    "    return batches\n",
    "\n",
    "print(\"âœ… ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã®å®šç¾©å®Œäº†\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "helpers"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title ğŸ”„ å‡¦ç†ãƒ¢ãƒ¼ãƒ‰é–¢æ•°å®šç¾©\n\n# ãƒãƒƒãƒå˜ä½ã®ãƒªãƒˆãƒ©ã‚¤è¨­å®š\nBATCH_MAX_RETRIES = 2   # åˆè¨ˆ3å›è©¦è¡Œ\nBATCH_RETRY_DELAY = 15  # ç§’\n\ndef call_batch_with_retry(fn, label: str):\n    \"\"\"ãƒãƒƒãƒå‡¦ç†ã®ãƒªãƒˆãƒ©ã‚¤ãƒ©ãƒƒãƒ‘ãƒ¼\"\"\"\n    for attempt in range(BATCH_MAX_RETRIES + 1):\n        try:\n            return fn()\n        except Exception as e:\n            if attempt == BATCH_MAX_RETRIES:\n                print(f\"âŒ {label} ãŒ {BATCH_MAX_RETRIES + 1} å›ã®è©¦è¡Œå¾Œã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n                raise\n            print(f\"âš ï¸  {label} ãŒå¤±æ•—ã—ã¾ã—ãŸ (è©¦è¡Œ {attempt + 1}/{BATCH_MAX_RETRIES + 1}): {e}\")\n            print(f\"   {BATCH_RETRY_DELAY}ç§’å¾Œã«å†è©¦è¡Œã—ã¾ã™...\")\n            time.sleep(BATCH_RETRY_DELAY)\n\ndef process_sequential_mode(\n    batches: List[List[str]],\n    messages_df: pd.DataFrame,\n    model_name: str,\n    temperature: float,\n    max_output_tokens: int,\n    interview_metadata: Dict[str, str]\n) -> str:\n    \"\"\"\n    é€æ¬¡å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: ãƒãƒƒãƒã‚’1ã¤ãšã¤å‡¦ç†\n    \"\"\"\n    current_report = \"\"\n    \n    for batch_index, batch in enumerate(batches):\n        is_first_batch = batch_index == 0\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"ğŸ“¦ ãƒãƒƒãƒ {batch_index + 1}/{len(batches)} å‡¦ç†ä¸­...\")\n        print(f\"{'='*60}\\n\")\n        \n        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿å–å¾—\n        sessions_data = get_session_data(messages_df, batch)\n        sessions_text = format_sessions_data(sessions_data)\n        \n        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ\n        output_length_guidance = \"Target output length: 10,000-20,000 characters per batch. Provide detailed analysis with extensive quotations.\"\n        \n        if is_first_batch:\n            print(\"ğŸ¤– åˆå›ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...\")\n            prompt = INITIAL_PROMPT_TEMPLATE\n        else:\n            print(\"ğŸ”„ ãƒ¬ãƒãƒ¼ãƒˆæ›´æ–°ä¸­...\")\n            prompt = UPDATE_PROMPT_TEMPLATE.replace('{{previousReport}}', current_report)\n            prompt = prompt.replace('{{newSessionCount}}', str(len(sessions_data)))\n        \n        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’åŸ‹ã‚è¾¼ã¿\n        for key, value in interview_metadata.items():\n            prompt = prompt.replace(f\"{{{{{key}}}}}\", value)\n        \n        prompt = prompt.replace('{{sessionCount}}', str(len(sessions_data)))\n        prompt = prompt.replace('{{sessionsData}}', sessions_text)\n        prompt = prompt.replace('{{outputLengthGuidance}}', output_length_guidance)\n        \n        estimated_tokens = estimate_tokens(prompt)\n        print(f\"   æ¨å®šå…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {estimated_tokens:,}\")\n        print(f\"   ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {len(sessions_data)}\")\n        \n        # APIå‘¼ã³å‡ºã—ï¼ˆãƒãƒƒãƒå˜ä½ã®ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰\n        result = call_batch_with_retry(\n            lambda: generate_text_with_gemini(\n                model_name=model_name,\n                prompt=prompt,\n                temperature=temperature,\n                max_output_tokens=max_output_tokens\n            ),\n            f\"ãƒãƒƒãƒ {batch_index + 1} LLMå‘¼ã³å‡ºã— (sequential)\"\n        )\n        \n        current_report = result['text']\n        usage = result['usage']\n        \n        print(f\"\\nâœ… ãƒãƒƒãƒ {batch_index + 1} å®Œäº†!\")\n        print(f\"   å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {usage['prompt_tokens']:,}\")\n        print(f\"   å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {usage['completion_tokens']:,}\")\n        print(f\"   åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³: {usage['total_tokens']:,}\")\n        print(f\"   å‡ºåŠ›ã‚µã‚¤ã‚º: {len(current_report):,} æ–‡å­—\")\n    \n    return current_report\n\ndef process_parallel_mode(\n    batches: List[List[str]],\n    messages_df: pd.DataFrame,\n    model_name: str,\n    temperature: float,\n    max_output_tokens: int,\n    interview_metadata: Dict[str, str]\n) -> str:\n    \"\"\"\n    ä¸¦åˆ—å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: å…¨ãƒãƒƒãƒã‚’åŒæ™‚å‡¦ç†å¾Œã«ãƒãƒ¼ã‚¸\n    \"\"\"\n    print(\"ğŸš€ ä¸¦åˆ—å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: å…¨ãƒãƒƒãƒã‚’å‡¦ç†ä¸­...\\n\")\n    \n    batch_reports = []\n    \n    # å„ãƒãƒƒãƒã‚’å‡¦ç†ï¼ˆå®Ÿéš›ã«ã¯Colabã§ã¯é †æ¬¡å®Ÿè¡Œã ãŒã€ç‹¬ç«‹ã—ã¦å‡¦ç†ï¼‰\n    for batch_index, batch in enumerate(batches):\n        print(f\"\\n{'='*60}\")\n        print(f\"ğŸ“¦ ãƒãƒƒãƒ {batch_index + 1}/{len(batches)} å‡¦ç†ä¸­...\")\n        print(f\"{'='*60}\\n\")\n        \n        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿å–å¾—\n        sessions_data = get_session_data(messages_df, batch)\n        sessions_text = format_sessions_data(sessions_data)\n        \n        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆï¼ˆå¸¸ã«INITIALãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½¿ç”¨ï¼‰\n        output_length_guidance = \"Target output length: 10,000-20,000 characters per batch. Provide detailed analysis with extensive quotations.\"\n        \n        prompt = INITIAL_PROMPT_TEMPLATE\n        \n        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’åŸ‹ã‚è¾¼ã¿\n        for key, value in interview_metadata.items():\n            prompt = prompt.replace(f\"{{{{{key}}}}}\", value)\n        \n        prompt = prompt.replace('{{sessionCount}}', str(len(sessions_data)))\n        prompt = prompt.replace('{{sessionsData}}', sessions_text)\n        prompt = prompt.replace('{{outputLengthGuidance}}', output_length_guidance)\n        \n        estimated_tokens = estimate_tokens(prompt)\n        print(f\"   æ¨å®šå…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {estimated_tokens:,}\")\n        print(f\"   ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {len(sessions_data)}\")\n        \n        # APIå‘¼ã³å‡ºã—ï¼ˆãƒãƒƒãƒå˜ä½ã®ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰\n        result = call_batch_with_retry(\n            lambda: generate_text_with_gemini(\n                model_name=model_name,\n                prompt=prompt,\n                temperature=temperature,\n                max_output_tokens=max_output_tokens\n            ),\n            f\"ãƒãƒƒãƒ {batch_index + 1} LLMå‘¼ã³å‡ºã— (parallel)\"\n        )\n        \n        batch_reports.append(result['text'])\n        usage = result['usage']\n        \n        print(f\"\\nâœ… ãƒãƒƒãƒ {batch_index + 1} å®Œäº†!\")\n        print(f\"   å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {usage['prompt_tokens']:,}\")\n        print(f\"   å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {usage['completion_tokens']:,}\")\n        print(f\"   åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³: {usage['total_tokens']:,}\")\n        print(f\"   å‡ºåŠ›ã‚µã‚¤ã‚º: {len(result['text']):,} æ–‡å­—\")\n    \n    # ãƒãƒ¼ã‚¸å‡¦ç†\n    print(f\"\\n{'='*60}\")\n    print(f\"ğŸ”€ {len(batch_reports)} å€‹ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒãƒ¼ã‚¸ä¸­...\")\n    print(f\"{'='*60}\\n\")\n    \n    combined_batch_reports = \"\\n\\n\".join([\n        f\"## ãƒãƒƒãƒ {i+1} ã®åˆ†æçµæœ\\n\\n{report}\"\n        for i, report in enumerate(batch_reports)\n    ])\n    \n    merge_output_length_guidance = \"Target output length: Comprehensive report with all information from batch reports preserved. Length may exceed individual batch reports to ensure no information is lost.\"\n    \n    merge_prompt = MERGE_PROMPT_TEMPLATE\n    \n    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’åŸ‹ã‚è¾¼ã¿\n    for key, value in interview_metadata.items():\n        merge_prompt = merge_prompt.replace(f\"{{{{{key}}}}}\", value)\n    \n    merge_prompt = merge_prompt.replace('{{batchCount}}', str(len(batch_reports)))\n    merge_prompt = merge_prompt.replace('{{batchReports}}', combined_batch_reports)\n    merge_prompt = merge_prompt.replace('{{outputLengthGuidance}}', merge_output_length_guidance)\n    \n    estimated_merge_tokens = estimate_tokens(merge_prompt)\n    print(f\"   æ¨å®šå…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {estimated_merge_tokens:,}\")\n    \n    # ãƒãƒ¼ã‚¸APIå‘¼ã³å‡ºã—ï¼ˆãƒãƒƒãƒå˜ä½ã®ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰\n    merge_result = call_batch_with_retry(\n        lambda: generate_text_with_gemini(\n            model_name=model_name,\n            prompt=merge_prompt,\n            temperature=temperature,\n            max_output_tokens=max_output_tokens\n        ),\n        \"ãƒãƒ¼ã‚¸ LLMå‘¼ã³å‡ºã—\"\n    )\n    \n    final_report = merge_result['text']\n    merge_usage = merge_result['usage']\n    \n    print(f\"\\nâœ… ãƒãƒ¼ã‚¸å®Œäº†!\")\n    print(f\"   å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {merge_usage['prompt_tokens']:,}\")\n    print(f\"   å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {merge_usage['completion_tokens']:,}\")\n    print(f\"   åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³: {merge_usage['total_tokens']:,}\")\n    print(f\"   å‡ºåŠ›ã‚µã‚¤ã‚º: {len(final_report):,} æ–‡å­—\")\n    \n    return final_report\n\nprint(\"âœ… å‡¦ç†ãƒ¢ãƒ¼ãƒ‰é–¢æ•°ã®å®šç¾©å®Œäº†\")",
   "metadata": {
    "cellView": "form",
    "id": "processing"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title â–¶ï¸ ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"ğŸš€ åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™\\n\")\n",
    "\n",
    "# Google Generative AI è¨­å®š\n",
    "configure_genai(GOOGLE_API_KEY)\n",
    "\n",
    "# CSVèª­ã¿è¾¼ã¿\n",
    "messages_df = load_messages_csv(MESSAGES_CSV_PATH)\n",
    "\n",
    "# ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "valid_session_ids = filter_sessions(messages_df, slug, MIN_MESSAGES)\n",
    "\n",
    "if len(valid_session_ids) == 0:\n",
    "    raise ValueError(f\"âŒ æ¡ä»¶ã«åˆã†ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆslug: {slug}, æœ€å°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {MIN_MESSAGES}ï¼‰\")\n",
    "\n",
    "# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "message_counts = messages_df[messages_df['session_id'].isin(valid_session_ids)].groupby('session_id').size().to_dict()\n",
    "\n",
    "# ãƒãƒƒãƒåˆ†å‰²\n",
    "batches = create_batches(\n",
    "    valid_session_ids,\n",
    "    message_counts,\n",
    "    TOKENS_PER_BATCH,\n",
    "    ESTIMATED_PROMPT_OVERHEAD\n",
    ")\n",
    "\n",
    "# ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆç°¡ç•¥ç‰ˆ - å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„ï¼‰\n",
    "interview_metadata = {\n",
    "    \"interviewTitle\": \"äººå·¥çŸ¥èƒ½åŸºæœ¬è¨ˆç”»ã«å¯¾ã™ã‚‹æè¨€\",\n",
    "    \"interviewDescription\": \"æ—¥æœ¬ã®AIæˆ¦ç•¥ã«é–¢ã™ã‚‹ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼\",\n",
    "    \"interviewOverview\": \"AIæŠ€è¡“ã®ç™ºå±•ã¨æ”¿ç­–ã«é–¢ã™ã‚‹å°‚é–€å®¶ã®æ„è¦‹ã‚’åé›†\",\n",
    "    \"interviewThemes\": \"1. AIæŠ•è³‡\\n2. äººæè‚²æˆ\\n3. ãƒ‡ãƒ¼ã‚¿æ•´å‚™\\n4. è¦åˆ¶ã¨ãƒãƒ©ãƒ³ã‚¹\",\n",
    "    \"interviewQuestions\": \"1. AIæŠ•è³‡ã®å„ªå…ˆé †ä½ã«ã¤ã„ã¦\\n2. äººæè‚²æˆã®èª²é¡Œ\\n3. ãƒ‡ãƒ¼ã‚¿æ•´å‚™ã®é‡è¦æ€§\",\n",
    "    \"knowledgeContext\": \"æ—¥æœ¬ã®AIæ”¿ç­–ã€å›½éš›æ¯”è¼ƒã€æŠ€è¡“å‹•å‘ã«é–¢ã™ã‚‹èƒŒæ™¯çŸ¥è­˜\"\n",
    "}\n",
    "\n",
    "# å‡¦ç†ãƒ¢ãƒ¼ãƒ‰é¸æŠ\n",
    "if mode == \"parallel\":\n",
    "    final_report = process_parallel_mode(\n",
    "        batches,\n",
    "        messages_df,\n",
    "        MODEL_NAME,\n",
    "        TEMPERATURE,\n",
    "        MAX_OUTPUT_TOKENS,\n",
    "        interview_metadata\n",
    "    )\n",
    "else:\n",
    "    final_report = process_sequential_mode(\n",
    "        batches,\n",
    "        messages_df,\n",
    "        MODEL_NAME,\n",
    "        TEMPERATURE,\n",
    "        MAX_OUTPUT_TOKENS,\n",
    "        interview_metadata\n",
    "    )\n",
    "\n",
    "# çµæœä¿å­˜\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_filename = f\"aggregate_report_{slug}_{timestamp}.md\"\n",
    "\n",
    "# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãã§ä¿å­˜\n",
    "report_with_metadata = f\"\"\"---\n",
    "Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "Model: {MODEL_NAME}\n",
    "Slug: {slug}\n",
    "SessionCount: {len(valid_session_ids)}\n",
    "Mode: {mode}\n",
    "Temperature: {TEMPERATURE}\n",
    "MinMessages: {MIN_MESSAGES}\n",
    "Memo: {memo if memo else \"N/A\"}\n",
    "---\n",
    "\n",
    "{final_report}\n",
    "\"\"\"\n",
    "\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(report_with_metadata)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… åˆ†æå®Œäº†!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ“Š çµ±è¨ˆæƒ…å ±:\")\n",
    "print(f\"   ç·ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {len(valid_session_ids):,}\")\n",
    "print(f\"   ãƒãƒƒãƒæ•°: {len(batches)}\")\n",
    "print(f\"   æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚µã‚¤ã‚º: {len(final_report):,} æ–‡å­—\")\n",
    "print(f\"   å‡¦ç†æ™‚é–“: {elapsed_time:.1f} ç§’\")\n",
    "print(f\"\\nğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {output_filename}\")\n",
    "print(\"\\nğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
    "\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "files.download(output_filename)\n",
    "\n",
    "print(\"\\nâœ¨ ã™ã¹ã¦å®Œäº†ã—ã¾ã—ãŸï¼\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "main"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}